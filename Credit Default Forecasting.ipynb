{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74647f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b35e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id  credit_lines_outstanding  loan_amt_outstanding  \\\n",
      "0      8153374                         0           5221.545193   \n",
      "1      7442532                         5           1958.928726   \n",
      "2      2256073                         0           3363.009259   \n",
      "3      4885975                         0           4766.648001   \n",
      "4      4700614                         1           1345.827718   \n",
      "\n",
      "   total_debt_outstanding       income  years_employed  fico_score  default  \n",
      "0             3915.471226  78039.38546               5         605        0  \n",
      "1             8228.752520  26648.43525               2         572        1  \n",
      "2             2027.830850  65866.71246               4         602        0  \n",
      "3             2501.730397  74356.88347               5         612        0  \n",
      "4             1768.826187  23448.32631               6         631        0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   customer_id               10000 non-null  int64  \n",
      " 1   credit_lines_outstanding  10000 non-null  int64  \n",
      " 2   loan_amt_outstanding      10000 non-null  float64\n",
      " 3   total_debt_outstanding    10000 non-null  float64\n",
      " 4   income                    10000 non-null  float64\n",
      " 5   years_employed            10000 non-null  int64  \n",
      " 6   fico_score                10000 non-null  int64  \n",
      " 7   default                   10000 non-null  int64  \n",
      "dtypes: float64(3), int64(5)\n",
      "memory usage: 625.1 KB\n",
      "None\n",
      "Logistic Regression AUC: 1.000\n",
      "Random Forest AUC: 1.000\n",
      "Expected Loss: $270.00\n"
     ]
    }
   ],
   "source": [
    "# Load the data (assuming the CSV filename is 'loan_data.csv')\n",
    "data = pd.read_csv(\"/Users/gadimg/Downloads/Task 3 and 4_Loan_Data.csv\")\n",
    "\n",
    "# Display basic information about the data\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=['customer_id', 'default'])  # Drop customer_id and target column\n",
    "y = data['default']  # Target column\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model evaluation on test data\n",
    "logistic_preds = logistic_model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for class 1\n",
    "rf_preds = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate AUC for model comparison\n",
    "logistic_auc = roc_auc_score(y_test, logistic_preds)\n",
    "rf_auc = roc_auc_score(y_test, rf_preds)\n",
    "\n",
    "print(f\"Logistic Regression AUC: {logistic_auc:.3f}\")\n",
    "print(f\"Random Forest AUC: {rf_auc:.3f}\")\n",
    "\n",
    "# Set recovery rate\n",
    "recovery_rate = 0.1\n",
    "\n",
    "def expected_loss(features, loan_amount, model=rf_model):\n",
    "    \"\"\"\n",
    "    Calculates the expected loss for a given set of borrower features.\n",
    "\n",
    "    Parameters:\n",
    "    - features: A dictionary of borrower characteristics (must match training feature names).\n",
    "    - loan_amount: The loan amount in currency units.\n",
    "    - model: The trained model to use for predicting default probability (default is rf_model).\n",
    "\n",
    "    Returns:\n",
    "    - Expected loss on the loan.\n",
    "    \"\"\"\n",
    "    # Convert features to DataFrame and scale\n",
    "    features_df = pd.DataFrame([features])\n",
    "    features_scaled = scaler.transform(features_df)\n",
    "\n",
    "    # Predict probability of default (PD)\n",
    "    pd_prob = model.predict_proba(features_scaled)[:, 1][0]\n",
    "\n",
    "    # Calculate expected loss\n",
    "    expected_loss_value = loan_amount * pd_prob * (1 - recovery_rate)\n",
    "    return expected_loss_value\n",
    "\n",
    "# Sample borrower characteristics (using provided column names)\n",
    "sample_borrower = {\n",
    "    'credit_lines': 3,\n",
    "    'loan_amt_out': 3000,\n",
    "    'total_debt_out': 5000,\n",
    "    'income': 45000,\n",
    "    'years_emplo': 4,\n",
    "    'fico_score': 650\n",
    "}\n",
    "\n",
    "# Example loan amount\n",
    "loan_amount = 10000\n",
    "\n",
    "# Calculate expected loss\n",
    "loss = expected_loss(sample_borrower, loan_amount, model=rf_model)\n",
    "print(f\"Expected Loss: ${loss:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515ea4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id  credit_lines_outstanding  loan_amt_outstanding  \\\n",
      "0      8153374                         0           5221.545193   \n",
      "1      7442532                         5           1958.928726   \n",
      "2      2256073                         0           3363.009259   \n",
      "3      4885975                         0           4766.648001   \n",
      "4      4700614                         1           1345.827718   \n",
      "\n",
      "   total_debt_outstanding       income  years_employed  fico_score  default  \\\n",
      "0             3915.471226  78039.38546               5         605        0   \n",
      "1             8228.752520  26648.43525               2         572        1   \n",
      "2             2027.830850  65866.71246               4         602        0   \n",
      "3             2501.730397  74356.88347               5         612        0   \n",
      "4             1768.826187  23448.32631               6         631        0   \n",
      "\n",
      "   bucket  bucket_center  \n",
      "0       0     582.786697  \n",
      "1       0     582.786697  \n",
      "2       0     582.786697  \n",
      "3       2     629.907583  \n",
      "4       2     629.907583  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def create_buckets_mse(data, num_buckets):\n",
    "    \"\"\"\n",
    "    Quantizes FICO scores into buckets using MSE minimization (K-means clustering).\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with columns 'fico_score' and 'default'\n",
    "    - num_buckets: Number of buckets to create\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with FICO scores and their corresponding bucket\n",
    "    \"\"\"\n",
    "    fico_scores = data[['fico_score']].values\n",
    "\n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_buckets, random_state=42)\n",
    "    data['bucket'] = kmeans.fit_predict(fico_scores)\n",
    "\n",
    "    # Map each bucket to the mean FICO score within that bucket\n",
    "    bucket_centers = kmeans.cluster_centers_.flatten()\n",
    "    data['bucket_center'] = data['bucket'].map(dict(enumerate(bucket_centers)))\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "num_buckets = 5  # Example: divide into 5 buckets\n",
    "bucketed_data_mse = create_buckets_mse(data, num_buckets)\n",
    "print(bucketed_data_mse.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56248d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data, boundaries):\n",
    "    \"\"\"\n",
    "    Calculate the log-likelihood of defaults given bucket boundaries.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with columns 'fico_score' and 'default'\n",
    "    - boundaries: List of bucket boundaries for FICO scores\n",
    "\n",
    "    Returns:\n",
    "    - Log-likelihood of the data given the bucket boundaries\n",
    "    \"\"\"\n",
    "    log_likelihood_value = 0\n",
    "    data = data.sort_values(by='fico_score')\n",
    "\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        # Define the bucket range\n",
    "        lower_bound, upper_bound = boundaries[i], boundaries[i + 1]\n",
    "\n",
    "        # Filter data for this bucket\n",
    "        bucket_data = data[(data['fico_score'] >= lower_bound) & (data['fico_score'] < upper_bound)]\n",
    "        ni = len(bucket_data)\n",
    "        ki = bucket_data['default'].sum()\n",
    "\n",
    "        # Calculate probability of default in the bucket\n",
    "        if ni > 0:\n",
    "            pi = ki / ni\n",
    "            if pi > 0 and pi < 1:  # To avoid log(0) or log(1)\n",
    "                log_likelihood_value += ki * np.log(pi) + (ni - ki) * np.log(1 - pi)\n",
    "\n",
    "    return log_likelihood_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7522a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Boundaries: [408.         520.20410629 585.00954142 645.54499113 730.76873523\n",
      " 850.        ]\n"
     ]
    }
   ],
   "source": [
    "def optimize_buckets_log_likelihood(data, num_buckets):\n",
    "    \"\"\"\n",
    "    Finds the optimal bucket boundaries for FICO scores to maximize log-likelihood.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with columns 'fico_score' and 'default'\n",
    "    - num_buckets: Number of buckets to create\n",
    "\n",
    "    Returns:\n",
    "    - List of optimized bucket boundaries\n",
    "    \"\"\"\n",
    "    # Initial equally spaced boundaries\n",
    "    min_score, max_score = data['fico_score'].min(), data['fico_score'].max()\n",
    "    boundaries = np.linspace(min_score, max_score, num_buckets + 1)\n",
    "\n",
    "    # Iterative approach to optimize boundaries\n",
    "    best_boundaries = boundaries\n",
    "    best_log_likelihood = log_likelihood(data, best_boundaries)\n",
    "\n",
    "    for _ in range(100):  # Limit iterations for simplicity\n",
    "        new_boundaries = best_boundaries.copy()\n",
    "        \n",
    "        # Adjust boundaries randomly within a small range\n",
    "        for i in range(1, len(new_boundaries) - 1):\n",
    "            adjustment = np.random.uniform(-10, 10)\n",
    "            new_boundaries[i] = np.clip(new_boundaries[i] + adjustment, min_score, max_score)\n",
    "        \n",
    "        # Calculate log-likelihood with new boundaries\n",
    "        new_log_likelihood = log_likelihood(data, new_boundaries)\n",
    "        \n",
    "        # Update if log-likelihood improves\n",
    "        if new_log_likelihood > best_log_likelihood:\n",
    "            best_log_likelihood = new_log_likelihood\n",
    "            best_boundaries = new_boundaries\n",
    "    \n",
    "    return best_boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bacb99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
