{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b741fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9bfdeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv('/Users/gadimg/Library/Mobile Documents/com~apple~CloudDocs/PhD dis/third/Data/Merged_data/Full_with_monthly_no_other_crypto.csv', parse_dates=['Date'], dayfirst=True)\n",
    "\n",
    "# Drop unnecessary columns and handle missing data\n",
    "df = df.drop(columns=['Date', 'WETH_Price'])\n",
    "df.replace(0, 1e-9, inplace=True)  # Handle zeros for percent change\n",
    "\n",
    "# Calculate percentage changes\n",
    "df_returns = df.pct_change().dropna()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_returns), columns=df_returns.columns)\n",
    "\n",
    "# Prepare the data\n",
    "Y = df_normalized['ETH_Price'].values\n",
    "X = df_normalized.drop(columns=['ETH_Price']).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2562aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.21554796341339127\n",
      "MAE: 0.1485734101828746\n",
      "MDA: 85.85858585858585%\n",
      "    ETH_Volume  BTC_Price  BTC_Volume  LINK_number_of_transfers  \\\n",
      "0     0.030285   0.353365   -0.006688                      -0.0   \n",
      "1     0.030082   0.353465   -0.006499                      -0.0   \n",
      "2     0.030082   0.353466   -0.006496                      -0.0   \n",
      "3     0.029941   0.353759   -0.006407                      -0.0   \n",
      "4     0.028931   0.355472   -0.007022                      -0.0   \n",
      "..         ...        ...         ...                       ...   \n",
      "95    0.000000   0.355171   -0.000000                      -0.0   \n",
      "96    0.000000   0.355186   -0.000000                      -0.0   \n",
      "97    0.000000   0.355367   -0.000000                      -0.0   \n",
      "98    0.000000   0.355398   -0.000000                      -0.0   \n",
      "99    0.000000   0.355347   -0.000000                      -0.0   \n",
      "\n",
      "    LINK_on_chain_volume  LINK_Price  LINK_Volume  \\\n",
      "0                   -0.0    0.336472         -0.0   \n",
      "1                   -0.0    0.336446         -0.0   \n",
      "2                   -0.0    0.336446         -0.0   \n",
      "3                   -0.0    0.336212         -0.0   \n",
      "4                   -0.0    0.334262         -0.0   \n",
      "..                   ...         ...          ...   \n",
      "95                  -0.0    0.323006         -0.0   \n",
      "96                  -0.0    0.323152         -0.0   \n",
      "97                  -0.0    0.323318         -0.0   \n",
      "98                  -0.0    0.323325         -0.0   \n",
      "99                  -0.0    0.323408         -0.0   \n",
      "\n",
      "    GWEI_average_daily_gas_price_gwei  WBTC_number_of_transfers  \\\n",
      "0                                 0.0                      -0.0   \n",
      "1                                 0.0                      -0.0   \n",
      "2                                 0.0                      -0.0   \n",
      "3                                 0.0                      -0.0   \n",
      "4                                 0.0                      -0.0   \n",
      "..                                ...                       ...   \n",
      "95                                0.0                      -0.0   \n",
      "96                                0.0                      -0.0   \n",
      "97                                0.0                      -0.0   \n",
      "98                                0.0                      -0.0   \n",
      "99                                0.0                      -0.0   \n",
      "\n",
      "    WBTC_on_chain_volume  ...  CHNMAINLANDEPU  NASDAQ_Close  NASDAQ_Volume  \\\n",
      "0                    0.0  ...             0.0      0.002062           -0.0   \n",
      "1                    0.0  ...             0.0      0.002024           -0.0   \n",
      "2                    0.0  ...             0.0      0.002023           -0.0   \n",
      "3                    0.0  ...             0.0      0.002121           -0.0   \n",
      "4                    0.0  ...             0.0      0.002028           -0.0   \n",
      "..                   ...  ...             ...           ...            ...   \n",
      "95                   0.0  ...             0.0      0.001484           -0.0   \n",
      "96                   0.0  ...             0.0      0.001495           -0.0   \n",
      "97                   0.0  ...             0.0      0.001531           -0.0   \n",
      "98                   0.0  ...             0.0      0.001499           -0.0   \n",
      "99                   0.0  ...             0.0      0.001625           -0.0   \n",
      "\n",
      "    TNX_Close  USEPUINDXD  FFR  NIKKEI225  DCOILBRENTEU  SP500  GOOGTREND  \n",
      "0        -0.0         0.0  0.0        0.0           0.0    0.0   0.018444  \n",
      "1        -0.0        -0.0  0.0        0.0           0.0    0.0   0.018353  \n",
      "2        -0.0        -0.0  0.0        0.0           0.0    0.0   0.018353  \n",
      "3        -0.0         0.0  0.0        0.0           0.0    0.0   0.018234  \n",
      "4        -0.0        -0.0  0.0        0.0           0.0    0.0   0.018495  \n",
      "..        ...         ...  ...        ...           ...    ...        ...  \n",
      "95       -0.0        -0.0  0.0        0.0           0.0    0.0   0.015312  \n",
      "96       -0.0        -0.0  0.0        0.0           0.0    0.0   0.015178  \n",
      "97       -0.0        -0.0  0.0        0.0           0.0    0.0   0.015239  \n",
      "98       -0.0        -0.0  0.0        0.0           0.0    0.0   0.015235  \n",
      "99       -0.0        -0.0  0.0        0.0           0.0    0.0   0.015270  \n",
      "\n",
      "[100 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "# Number of observations\n",
    "n = len(df_normalized)\n",
    "start = n - 100\n",
    "predictionsLasso = []\n",
    "coefficients = []\n",
    "\n",
    "# Recursive window prediction with time series cross-validation\n",
    "for i in range(start, n):\n",
    "    # Define the training data\n",
    "    X_train = X[:i]\n",
    "    y_train = Y[:i]\n",
    "\n",
    "    # TimeSeriesSplit for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    model = LassoCV(cv=tscv, normalize=False, random_state=0)  # Set normalize to False since we've already normalized\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save coefficients\n",
    "    coefficients.append(model.coef_)\n",
    "\n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X[i].reshape(1, -1))\n",
    "    predictionsLasso.append(y_pred[0])\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(Y[start:], predictionsLasso))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "actuals = Y[start:]\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(actuals, predictionsLasso)\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Calculate MDA\n",
    "actuals_array = np.array(actuals)\n",
    "predictionsLasso_array = np.array(predictionsLasso)\n",
    "correct_direction = np.sign(actuals_array[1:] - actuals_array[:-1]) == np.sign(predictionsLasso_array[1:] - predictionsLasso_array[:-1])\n",
    "mda = np.mean(correct_direction) * 100\n",
    "print(f\"MDA: {mda}%\")\n",
    "\n",
    "# Convert coefficients to a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame(coefficients, columns=df.drop(columns=['ETH_Price']).columns)\n",
    "print(coef_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b2f4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_coef = coef_df.iloc[-1]\n",
    "\n",
    "# Get the sorted column names based on the absolute values of the coefficients from the last row\n",
    "sorted_columns = last_coef.abs().sort_values(ascending=False).index\n",
    "\n",
    "# Reorder the entire DataFrame based on these sorted column names\n",
    "sorted_coef_df = coef_df[sorted_columns]\n",
    "\n",
    "# Export to CSV\n",
    "sorted_coef_df.to_csv('/Users/gadimg/Library/Mobile Documents/com~apple~CloudDocs/PhD dis/third/Results/pct_change_results/Lasso_test2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a6701e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.25726497033286033\n",
      "MAE: 0.20698559914630782\n",
      "MDA: 75.75757575757575%\n",
      "    ETH_Volume  BTC_Price  BTC_Volume  LINK_number_of_transfers  \\\n",
      "0     0.220423   0.257517   -0.107404                 -0.001821   \n",
      "1     0.220416   0.257504   -0.107416                 -0.001825   \n",
      "2     0.220446   0.257500   -0.107344                 -0.001830   \n",
      "3     0.220505   0.257637   -0.107089                 -0.001806   \n",
      "4     0.219855   0.258181   -0.107312                 -0.002536   \n",
      "..         ...        ...         ...                       ...   \n",
      "95    0.191219   0.258363   -0.104690                 -0.007136   \n",
      "96    0.192508   0.258306   -0.106848                 -0.007224   \n",
      "97    0.192723   0.258284   -0.106496                 -0.007079   \n",
      "98    0.192827   0.258266   -0.106409                 -0.007125   \n",
      "99    0.192624   0.258214   -0.106522                 -0.007107   \n",
      "\n",
      "    LINK_on_chain_volume  LINK_Price  LINK_Volume  \\\n",
      "0              -0.017353    0.300865     0.021908   \n",
      "1              -0.017354    0.300874     0.021911   \n",
      "2              -0.017361    0.300895     0.021880   \n",
      "3              -0.017383    0.300619     0.021802   \n",
      "4              -0.018029    0.299123     0.021755   \n",
      "..                   ...         ...          ...   \n",
      "95             -0.015524    0.291671     0.019275   \n",
      "96             -0.015332    0.291810     0.020360   \n",
      "97             -0.015389    0.291924     0.020169   \n",
      "98             -0.015278    0.291773     0.020134   \n",
      "99             -0.015290    0.291855     0.020211   \n",
      "\n",
      "    GWEI_average_daily_gas_price_gwei  WBTC_number_of_transfers  \\\n",
      "0                            0.011060                 -0.059832   \n",
      "1                            0.011056                 -0.059810   \n",
      "2                            0.011073                 -0.059729   \n",
      "3                            0.011121                 -0.059509   \n",
      "4                            0.011219                 -0.059218   \n",
      "..                                ...                       ...   \n",
      "95                           0.012100                 -0.049766   \n",
      "96                           0.012134                 -0.047164   \n",
      "97                           0.012183                 -0.047160   \n",
      "98                           0.012231                 -0.046596   \n",
      "99                           0.012229                 -0.046516   \n",
      "\n",
      "    WBTC_on_chain_volume  ...  CHNMAINLANDEPU  NASDAQ_Close  NASDAQ_Volume  \\\n",
      "0               0.017630  ...        0.015184      0.029567      -0.006411   \n",
      "1               0.017623  ...        0.015188      0.029568      -0.006407   \n",
      "2               0.017541  ...        0.015184      0.029520      -0.006394   \n",
      "3               0.017236  ...        0.015179      0.029626      -0.006493   \n",
      "4               0.016822  ...        0.015044      0.029364      -0.006304   \n",
      "..                   ...  ...             ...           ...            ...   \n",
      "95              0.015163  ...        0.014608      0.032975      -0.003532   \n",
      "96              0.015997  ...        0.014524      0.033098      -0.003284   \n",
      "97              0.016007  ...        0.014525      0.033189      -0.003329   \n",
      "98              0.013672  ...        0.014517      0.033183      -0.003377   \n",
      "99              0.013791  ...        0.014502      0.033178      -0.003440   \n",
      "\n",
      "    TNX_Close  USEPUINDXD       FFR  NIKKEI225  DCOILBRENTEU     SP500  \\\n",
      "0   -0.009930   -0.012768  0.011099   0.018614      0.010030 -0.002189   \n",
      "1   -0.009921   -0.012729  0.011100   0.018614      0.010017 -0.002185   \n",
      "2   -0.009917   -0.012629  0.011109   0.018613      0.010051 -0.002158   \n",
      "3   -0.009966   -0.012506  0.011114   0.018624      0.009900 -0.002188   \n",
      "4   -0.009920   -0.013255  0.010999   0.018454      0.010180 -0.002053   \n",
      "..        ...         ...       ...        ...           ...       ...   \n",
      "95  -0.011279   -0.011314  0.011069   0.015271      0.010550 -0.005057   \n",
      "96  -0.011302   -0.011086  0.010943   0.015447      0.010640 -0.005163   \n",
      "97  -0.011503   -0.010995  0.010997   0.015180      0.010698 -0.005224   \n",
      "98  -0.011488   -0.010932  0.011038   0.015224      0.010641 -0.005128   \n",
      "99  -0.011558   -0.011192  0.011016   0.015443      0.010631 -0.005027   \n",
      "\n",
      "    GOOGTREND  \n",
      "0    0.058336  \n",
      "1    0.058344  \n",
      "2    0.058343  \n",
      "3    0.058265  \n",
      "4    0.058475  \n",
      "..        ...  \n",
      "95   0.059864  \n",
      "96   0.060319  \n",
      "97   0.060286  \n",
      "98   0.060314  \n",
      "99   0.060344  \n",
      "\n",
      "[100 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the starting point and the number of predictions\n",
    "#Ridge regression with cryptos\n",
    "n = len(df_normalized) #add when need lagged one\n",
    "start = n - 100\n",
    "predictionsRidge = []\n",
    "coefficients = []\n",
    "\n",
    "# Define alphas (the regularization strengths to try)\n",
    "alphas = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "\n",
    "# Recursive window prediction\n",
    "for i in range(start, n):\n",
    "    # Define the training data\n",
    "    X_train = X[:i]\n",
    "    y_train = Y[:i]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    # Define Ridge with 10-fold CV\n",
    "    model = RidgeCV(alphas=alphas, cv=tscv, normalize=False)  # Set normalize to False since we've already normalized\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save coefficients\n",
    "    coefficients.append(model.coef_)\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X[i].reshape(1, -1))\n",
    "    predictionsRidge.append(y_pred[0])\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(Y[start:], predictionsRidge))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "actuals = Y[start:]\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(actuals, predictionsRidge)\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Calculate MDA\n",
    "actuals_array = np.array(actuals)\n",
    "predictionsRidge_array = np.array(predictionsRidge)\n",
    "correct_direction = np.sign(actuals_array[1:] - actuals_array[:-1]) == np.sign(predictionsRidge_array[1:] - predictionsRidge_array[:-1])\n",
    "mda = np.mean(correct_direction) * 100\n",
    "print(f\"MDA: {mda}%\")\n",
    "\n",
    "# Convert coefficients to a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame(coefficients, columns=df.drop(columns=['ETH_Price']).columns)\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "71080d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Lasso OLS RMSE: 0.18911130773075166\n",
      "MAE: 0.14166399401373314\n",
      "MDA: 85.85858585858585%\n"
     ]
    }
   ],
   "source": [
    "#post-lasso without Lags\n",
    "from sklearn.linear_model import LinearRegression\n",
    "n = len(df_normalized)\n",
    "start = n - 100\n",
    "\n",
    "non_zero_mask = coef_df.iloc[-1] != 0\n",
    "\n",
    "feature_columns = df_normalized.drop(columns=['ETH_Price']).columns\n",
    "significant_columns = feature_columns[non_zero_mask] \n",
    "\n",
    "X_significant = X[:, non_zero_mask]\n",
    "# Prepare to predict with OLS on the filtered X\n",
    "ols_predictions = []\n",
    "ols_model = LinearRegression()\n",
    "\n",
    "# Recursive window prediction using the filtered features\n",
    "for i in range(start, n):\n",
    "    X_train_significant = X_significant[:i]\n",
    "    y_train = Y[:i]\n",
    "    \n",
    "    # Fit the OLS model\n",
    "    ols_model.fit(X_train_significant, y_train)\n",
    "    \n",
    "    # Make prediction using the OLS model\n",
    "    y_pred_ols = ols_model.predict(X_significant[[i]])  # Ensure 2D input for the prediction\n",
    "    ols_predictions.append(y_pred_ols[0])\n",
    "\n",
    "# Calculate RMSE for the OLS predictions\n",
    "ols_rmse = np.sqrt(mean_squared_error(Y[start:], ols_predictions))\n",
    "print(f\"Post-Lasso OLS RMSE: {ols_rmse}\")\n",
    "\n",
    "actuals = Y[start:]\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(actuals, ols_predictions)\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Calculate MDA\n",
    "actuals_array = np.array(actuals)\n",
    "predictionsOLS_array = np.array(ols_predictions)\n",
    "correct_direction = np.sign(actuals_array[1:] - actuals_array[:-1]) == np.sign(predictionsOLS_array[1:] - predictionsOLS_array[:-1])\n",
    "mda = np.mean(correct_direction) * 100\n",
    "print(f\"MDA: {mda}%\")\n",
    "\n",
    "# Export the final OLS coefficients alongside their feature names to a # ...\n",
    "\n",
    "# Export the final OLS coefficients alongside their feature names to a CSV:\n",
    "final_ols_coef_df = pd.DataFrame({'Coefficient': ols_model.coef_}, index=significant_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faab92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final OLS coefficients alongside their feature names to a CSV:\n",
    "final_ols_coef_df = pd.DataFrame({'Coefficient': ols_model.coef_}, index=significant_columns)\n",
    "\n",
    "# Calculate the absolute values and sort by them in descending order\n",
    "final_ols_coef_df['Absolute_Coefficient'] = final_ols_coef_df['Coefficient'].abs()\n",
    "sorted_final_ols_coef_df = final_ols_coef_df.sort_values('Absolute_Coefficient', ascending=False).drop('Absolute_Coefficient', axis=1)\n",
    "\n",
    "# Export the sorted DataFrame to CSV\n",
    "sorted_final_ols_coef_df.to_csv('/Users/gadimg/Library/Mobile Documents/com~apple~CloudDocs/PhD dis/third/Results/pct_change_results/postLasso_withOUTcrypto_pct_change.csv', index=True, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3da9f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              ETH_Price   R-squared:                       0.784\n",
      "Model:                            OLS   Adj. R-squared:                  0.782\n",
      "Method:                 Least Squares   F-statistic:                     463.2\n",
      "Date:                Sun, 11 Aug 2024   Prob (F-statistic):               0.00\n",
      "Time:                        22:30:54   Log-Likelihood:                -759.77\n",
      "No. Observations:                1162   AIC:                             1540.\n",
      "Df Residuals:                    1152   BIC:                             1590.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const       -7.589e-18      0.014  -5.54e-16      1.000      -0.027       0.027\n",
      "BTC_Price       0.3259      0.095      3.427      0.001       0.139       0.512\n",
      "LINK_Price      0.2405      0.024     10.216      0.000       0.194       0.287\n",
      "WBTC_Price      0.0603      0.095      0.638      0.524      -0.125       0.246\n",
      "MATIC_Price     0.0790      0.020      3.953      0.000       0.040       0.118\n",
      "TRX_Close       0.0600      0.019      3.237      0.001       0.024       0.096\n",
      "DOT_Close       0.0828      0.022      3.844      0.000       0.041       0.125\n",
      "ADA_Close       0.0687      0.021      3.196      0.001       0.027       0.111\n",
      "BNB_Close       0.0268      0.020      1.328      0.184      -0.013       0.066\n",
      "SOL_Close       0.1211      0.018      6.784      0.000       0.086       0.156\n",
      "==============================================================================\n",
      "Omnibus:                      470.163   Durbin-Watson:                   1.947\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6446.051\n",
      "Skew:                           1.481   Prob(JB):                         0.00\n",
      "Kurtosis:                      14.152   Cond. No.                         23.4\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "const          1.000000e+00\n",
      "BTC_Price      6.307894e-04\n",
      "LINK_Price     1.634143e-23\n",
      "WBTC_Price     5.238472e-01\n",
      "MATIC_Price    8.174436e-05\n",
      "TRX_Close      1.240229e-03\n",
      "DOT_Close      1.275299e-04\n",
      "ADA_Close      1.432270e-03\n",
      "BNB_Close      1.844746e-01\n",
      "SOL_Close      1.866984e-11\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#getting the p-values after for post lasso, full dataset\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming 'significant_columns' contains the names of the predictors selected by Lasso\n",
    "X_significant = df_normalized[significant_columns]\n",
    "Y = df_normalized['ETH_Price']\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X_significant = sm.add_constant(X_significant)\n",
    "\n",
    "# Fit the OLS model using statsmodels\n",
    "ols_model = sm.OLS(Y, X_significant).fit()\n",
    "\n",
    "# Print out the summary which includes the p-values\n",
    "print(ols_model.summary())\n",
    "\n",
    "# If you need to access the p-values directly\n",
    "p_values = ols_model.pvalues\n",
    "print(p_values)\n",
    "\n",
    "# To export the coefficients and p-values to a CSV:\n",
    "coeff_pvalues_df = pd.DataFrame({\n",
    "    'Coefficient': ols_model.params,\n",
    "    'P-Value': ols_model.pvalues\n",
    "})\n",
    "#coeff_pvalues_df.to_csv('/Users/gadimg/Library/Mobile Documents/com~apple~CloudDocs/PhD dis/third/Results/pct_change_results/postLasso_pct_change_p_values.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efccbb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>BTC_Price</th>\n",
       "      <th>LINK_Price</th>\n",
       "      <th>WBTC_Price</th>\n",
       "      <th>MATIC_Price</th>\n",
       "      <th>TRX_Close</th>\n",
       "      <th>DOT_Close</th>\n",
       "      <th>ADA_Close</th>\n",
       "      <th>BNB_Close</th>\n",
       "      <th>SOL_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.753813</td>\n",
       "      <td>-2.492531</td>\n",
       "      <td>-0.898223</td>\n",
       "      <td>-0.967097</td>\n",
       "      <td>-2.521512</td>\n",
       "      <td>-0.176143</td>\n",
       "      <td>-1.424152</td>\n",
       "      <td>-0.863420</td>\n",
       "      <td>-1.218470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.188220</td>\n",
       "      <td>2.702549</td>\n",
       "      <td>0.637723</td>\n",
       "      <td>0.590605</td>\n",
       "      <td>0.672509</td>\n",
       "      <td>9.258711</td>\n",
       "      <td>0.193629</td>\n",
       "      <td>0.034897</td>\n",
       "      <td>0.307163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.083347</td>\n",
       "      <td>-0.873470</td>\n",
       "      <td>-0.217642</td>\n",
       "      <td>-0.504259</td>\n",
       "      <td>-0.636028</td>\n",
       "      <td>-1.948252</td>\n",
       "      <td>-0.587783</td>\n",
       "      <td>-0.380268</td>\n",
       "      <td>1.339936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238684</td>\n",
       "      <td>-0.007819</td>\n",
       "      <td>0.283864</td>\n",
       "      <td>1.356140</td>\n",
       "      <td>0.562697</td>\n",
       "      <td>2.625932</td>\n",
       "      <td>0.410274</td>\n",
       "      <td>0.584606</td>\n",
       "      <td>0.034288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.068750</td>\n",
       "      <td>-1.124549</td>\n",
       "      <td>-0.796810</td>\n",
       "      <td>-1.091710</td>\n",
       "      <td>-1.764557</td>\n",
       "      <td>3.287260</td>\n",
       "      <td>-1.724320</td>\n",
       "      <td>-1.022605</td>\n",
       "      <td>-0.397213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.444144</td>\n",
       "      <td>0.257284</td>\n",
       "      <td>1.483932</td>\n",
       "      <td>0.420402</td>\n",
       "      <td>0.503195</td>\n",
       "      <td>0.105254</td>\n",
       "      <td>0.282939</td>\n",
       "      <td>0.404313</td>\n",
       "      <td>1.258420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.148081</td>\n",
       "      <td>-0.494695</td>\n",
       "      <td>-0.116464</td>\n",
       "      <td>-0.439378</td>\n",
       "      <td>-0.128953</td>\n",
       "      <td>-0.519991</td>\n",
       "      <td>-0.368152</td>\n",
       "      <td>-0.345964</td>\n",
       "      <td>-0.074822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.131455</td>\n",
       "      <td>0.051164</td>\n",
       "      <td>-0.169993</td>\n",
       "      <td>-0.281109</td>\n",
       "      <td>0.111015</td>\n",
       "      <td>-0.116361</td>\n",
       "      <td>-0.314426</td>\n",
       "      <td>-0.165102</td>\n",
       "      <td>-0.369791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.369443</td>\n",
       "      <td>-0.138068</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>0.097424</td>\n",
       "      <td>0.135430</td>\n",
       "      <td>-0.067600</td>\n",
       "      <td>0.231237</td>\n",
       "      <td>-0.002641</td>\n",
       "      <td>0.840888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.954014</td>\n",
       "      <td>0.625022</td>\n",
       "      <td>0.959992</td>\n",
       "      <td>0.411170</td>\n",
       "      <td>0.142438</td>\n",
       "      <td>0.296488</td>\n",
       "      <td>0.284107</td>\n",
       "      <td>0.043931</td>\n",
       "      <td>1.118455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1162 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      const  BTC_Price  LINK_Price  WBTC_Price  MATIC_Price  TRX_Close  \\\n",
       "0       1.0  -0.753813   -2.492531   -0.898223    -0.967097  -2.521512   \n",
       "1       1.0   0.188220    2.702549    0.637723     0.590605   0.672509   \n",
       "2       1.0  -0.083347   -0.873470   -0.217642    -0.504259  -0.636028   \n",
       "3       1.0   0.238684   -0.007819    0.283864     1.356140   0.562697   \n",
       "4       1.0  -1.068750   -1.124549   -0.796810    -1.091710  -1.764557   \n",
       "...     ...        ...         ...         ...          ...        ...   \n",
       "1157    1.0   1.444144    0.257284    1.483932     0.420402   0.503195   \n",
       "1158    1.0  -0.148081   -0.494695   -0.116464    -0.439378  -0.128953   \n",
       "1159    1.0  -0.131455    0.051164   -0.169993    -0.281109   0.111015   \n",
       "1160    1.0   0.369443   -0.138068    0.375931     0.097424   0.135430   \n",
       "1161    1.0   0.954014    0.625022    0.959992     0.411170   0.142438   \n",
       "\n",
       "      DOT_Close  ADA_Close  BNB_Close  SOL_Close  \n",
       "0     -0.176143  -1.424152  -0.863420  -1.218470  \n",
       "1      9.258711   0.193629   0.034897   0.307163  \n",
       "2     -1.948252  -0.587783  -0.380268   1.339936  \n",
       "3      2.625932   0.410274   0.584606   0.034288  \n",
       "4      3.287260  -1.724320  -1.022605  -0.397213  \n",
       "...         ...        ...        ...        ...  \n",
       "1157   0.105254   0.282939   0.404313   1.258420  \n",
       "1158  -0.519991  -0.368152  -0.345964  -0.074822  \n",
       "1159  -0.116361  -0.314426  -0.165102  -0.369791  \n",
       "1160  -0.067600   0.231237  -0.002641   0.840888  \n",
       "1161   0.296488   0.284107   0.043931   1.118455  \n",
       "\n",
       "[1162 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "053c0c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS RMSE: 0.3324955139587656\n",
      "MAE: 0.26160605541418236\n",
      "MDA: 67.67676767676768%\n",
      "    ETH_Volume  BTC_Price  BTC_Volume  LINK_number_of_transfers  \\\n",
      "0     0.405561   0.446521   -0.207346                 -0.000239   \n",
      "1     0.405523   0.446369   -0.207421                 -0.000259   \n",
      "2     0.405552   0.446360   -0.207287                 -0.000270   \n",
      "3     0.405791   0.446905   -0.206647                 -0.000225   \n",
      "4     0.404195   0.449226   -0.207311                 -0.001158   \n",
      "..         ...        ...         ...                       ...   \n",
      "95    0.317922   0.436848   -0.202894                 -0.008033   \n",
      "96    0.318811   0.436816   -0.203723                 -0.008063   \n",
      "97    0.319107   0.436925   -0.203094                 -0.007910   \n",
      "98    0.319298   0.436863   -0.202928                 -0.008014   \n",
      "99    0.318879   0.436839   -0.203187                 -0.007988   \n",
      "\n",
      "    LINK_on_chain_volume  LINK_Price  LINK_Volume  \\\n",
      "0              -0.020056    0.309888     0.064940   \n",
      "1              -0.020063    0.309929     0.064968   \n",
      "2              -0.020073    0.309954     0.064908   \n",
      "3              -0.020085    0.309595     0.064668   \n",
      "4              -0.020931    0.307487     0.064802   \n",
      "..                   ...         ...          ...   \n",
      "95             -0.017458    0.299343     0.060714   \n",
      "96             -0.017412    0.299386     0.061145   \n",
      "97             -0.017460    0.299525     0.060813   \n",
      "98             -0.017264    0.299208     0.060746   \n",
      "99             -0.017287    0.299334     0.060907   \n",
      "\n",
      "    GWEI_average_daily_gas_price_gwei  WBTC_number_of_transfers  \\\n",
      "0                            0.012713                 -0.077235   \n",
      "1                            0.012698                 -0.077123   \n",
      "2                            0.012721                 -0.076995   \n",
      "3                            0.012799                 -0.076634   \n",
      "4                            0.012910                 -0.076274   \n",
      "..                                ...                       ...   \n",
      "95                           0.013996                 -0.057417   \n",
      "96                           0.014021                 -0.056485   \n",
      "97                           0.014073                 -0.056520   \n",
      "98                           0.014169                 -0.055302   \n",
      "99                           0.014163                 -0.055177   \n",
      "\n",
      "    WBTC_on_chain_volume  ...  CHNMAINLANDEPU  NASDAQ_Close  NASDAQ_Volume  \\\n",
      "0               0.024909  ...        0.019997      0.044317      -0.008450   \n",
      "1               0.024878  ...        0.020011      0.044312      -0.008435   \n",
      "2               0.024762  ...        0.020004      0.044173      -0.008417   \n",
      "3               0.024342  ...        0.020000      0.044407      -0.008528   \n",
      "4               0.023717  ...        0.019799      0.043649      -0.008316   \n",
      "..                   ...  ...             ...           ...            ...   \n",
      "95              0.019993  ...        0.019166      0.053735      -0.004854   \n",
      "96              0.020228  ...        0.019139      0.053805      -0.004764   \n",
      "97              0.020236  ...        0.019146      0.054017      -0.004805   \n",
      "98              0.015976  ...        0.019131      0.053867      -0.004879   \n",
      "99              0.016151  ...        0.019099      0.053738      -0.004968   \n",
      "\n",
      "    TNX_Close  USEPUINDXD       FFR  NIKKEI225  DCOILBRENTEU     SP500  \\\n",
      "0   -0.007131   -0.023899  0.010834   0.022942      0.011138 -0.028348   \n",
      "1   -0.007095   -0.023743  0.010841   0.022944      0.011089 -0.028325   \n",
      "2   -0.007098   -0.023611  0.010854   0.022942      0.011129 -0.028212   \n",
      "3   -0.007159   -0.023465  0.010867   0.022958      0.010943 -0.028348   \n",
      "4   -0.007131   -0.024385  0.010691   0.022730      0.011267 -0.027789   \n",
      "..        ...         ...       ...        ...           ...       ...   \n",
      "95  -0.009464   -0.017450  0.011214   0.017457      0.011422 -0.037002   \n",
      "96  -0.009457   -0.017394  0.011178   0.017507      0.011446 -0.037078   \n",
      "97  -0.009649   -0.017304  0.011237   0.017234      0.011518 -0.037261   \n",
      "98  -0.009608   -0.017190  0.011317   0.017321      0.011400 -0.036994   \n",
      "99  -0.009718   -0.017556  0.011281   0.017634      0.011379 -0.036733   \n",
      "\n",
      "    GOOGTREND  \n",
      "0    0.056428  \n",
      "1    0.056458  \n",
      "2    0.056454  \n",
      "3    0.056351  \n",
      "4    0.056631  \n",
      "..        ...  \n",
      "95   0.061112  \n",
      "96   0.061220  \n",
      "97   0.061181  \n",
      "98   0.061219  \n",
      "99   0.061268  \n",
      "\n",
      "[100 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize lists to store predictions and coefficients\n",
    "ols_predictions = []\n",
    "ols_coefficients = []\n",
    "n = len(df_normalized)\n",
    "start = n - 100\n",
    "# Recursive window prediction\n",
    "for i in range(start, n):\n",
    "    # Define the training data\n",
    "    X_train = X[:i]\n",
    "    y_train = Y[:i]\n",
    "    \n",
    "    # Define and fit the OLS model\n",
    "    ols_model = LinearRegression()\n",
    "    ols_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save coefficients\n",
    "    ols_coefficients.append(ols_model.coef_)\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred_ols = ols_model.predict(X[i].reshape(1, -1))\n",
    "    ols_predictions.append(y_pred_ols[0])\n",
    "\n",
    "# Calculate RMSE for OLS predictions\n",
    "ols_rmse = np.sqrt(mean_squared_error(Y[start:], ols_predictions))\n",
    "print(f\"OLS RMSE: {ols_rmse}\")\n",
    "\n",
    "actuals = Y[start:]\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(actuals, ols_predictions)\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Calculate MDA\n",
    "actuals_array = np.array(actuals)\n",
    "predictionsOLS_array = np.array(ols_predictions)\n",
    "correct_direction = np.sign(actuals_array[1:] - actuals_array[:-1]) == np.sign(predictionsOLS_array[1:] - predictionsOLS_array[:-1])\n",
    "mda = np.mean(correct_direction) * 100\n",
    "print(f\"MDA: {mda}%\")\n",
    "\n",
    "# Convert OLS coefficients to a DataFrame for better visualization\n",
    "ols_coef_df = pd.DataFrame(ols_coefficients, columns=df.drop(columns=['ETH_Price']).columns)\n",
    "print(ols_coef_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "489ff27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
      "<ipython-input-21-32ca52ca0879>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n"
     ]
    }
   ],
   "source": [
    "#this did not yield the expected results\n",
    "def add_lags(df, n_lags):\n",
    "    df_lagged = df.copy()\n",
    "    for column in df.columns:\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            df_lagged[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
    "    # It will drop the rows where at least one element is missing\n",
    "    df_lagged = df_lagged.dropna()\n",
    "    return df_lagged\n",
    "\n",
    "# Number of lags to add\n",
    "n_lags = 7\n",
    "\n",
    "# Add lags to the data (excluding the 'ETH_Price' column)\n",
    "df_lagged = add_lags(df_normalized.drop(columns=['ETH_Price']), n_lags)\n",
    "\n",
    "# Align the 'ETH_Price' with the lagged features\n",
    "# Note that the shift is no longer needed since we are aligning by dropping rows from both the feature and target datasets\n",
    "df_lagged = df_lagged.reset_index(drop=True)\n",
    "df_target_aligned = df_normalized['ETH_Price'].iloc[n_lags:].reset_index(drop=True)\n",
    "\n",
    "# Ensure df_lagged and df_target_aligned have the same length\n",
    "assert len(df_lagged) == len(df_target_aligned), \"The features and target are not aligned.\"\n",
    "\n",
    "# Update Y and X with the aligned data\n",
    "Y = df_target_aligned.values\n",
    "X = df_lagged.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebb8351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.036597229759943006, tolerance: 0.024179168971507993\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.11636724392440101, tolerance: 0.0731672883428463\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10009829915289714, tolerance: 0.07332161065361116\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07810404087837952, tolerance: 0.07346860430622196\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09283299340787465, tolerance: 0.07346860430622196\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.11922121590329482, tolerance: 0.0731672883428463\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10201472773191256, tolerance: 0.07332161065361116\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0817398978474202, tolerance: 0.07346860430622196\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08941872228710679, tolerance: 0.07346860430622196\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13414889277911612, tolerance: 0.07347232002151019\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08334758283163879, tolerance: 0.07348647949243944\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13144903686066556, tolerance: 0.07348647949243944\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09361850688121365, tolerance: 0.07349399948914989\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13435949982035567, tolerance: 0.07347232002151019\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08659927205066253, tolerance: 0.07348647949243944\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12413314024856703, tolerance: 0.07348647949243944\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09507670504212129, tolerance: 0.07349399948914989\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08923886643182044, tolerance: 0.0738069167984578\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07321706117990567, tolerance: 0.05755711303431392\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09315725775069694, tolerance: 0.07383052574645045\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.080240326007889, tolerance: 0.07418574350449274\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08561068544182149, tolerance: 0.0738069167984578\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09514484599645812, tolerance: 0.07383052574645045\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07378612275499874, tolerance: 0.05755711303431392\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0773025529648308, tolerance: 0.07418574350449274\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08215070699777627, tolerance: 0.07425967482618802\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0809965868590723, tolerance: 0.07426809188037707\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09895311712792676, tolerance: 0.07441413813673853\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07438379634851855, tolerance: 0.05755711303431392\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08492204609453324, tolerance: 0.07425967482618802\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08350004486211304, tolerance: 0.07426809188037707\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10182244906240356, tolerance: 0.07441413813673853\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09992286300630226, tolerance: 0.07444754153279069\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06808819617141992, tolerance: 0.05781616172902848\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09087618665716235, tolerance: 0.07446887219179432\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09786811867669343, tolerance: 0.07444754153279069\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08952476487058547, tolerance: 0.07446887219179432\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0681456290411897, tolerance: 0.05781616172902848\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10772948797436399, tolerance: 0.07449669229220976\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10235571204852079, tolerance: 0.0746732463479301\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08196692141985906, tolerance: 0.07467330094331386\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06774813891659948, tolerance: 0.05781616172902848\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10970191363738735, tolerance: 0.07449669229220976\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10464380390031991, tolerance: 0.0746732463479301\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08448488808731014, tolerance: 0.07467330094331386\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09816888679490177, tolerance: 0.07486827085555217\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0775176501464756, tolerance: 0.07547055193099303\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09835410237459996, tolerance: 0.07486827085555217\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07808627203610996, tolerance: 0.07547055193099303\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08782892488734717, tolerance: 0.07547852795757067\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08989792305623467, tolerance: 0.07547852795757067\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06197786606507716, tolerance: 0.06039080487537683\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06496861205329552, tolerance: 0.06039080487537683\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.18636981639005254\n",
      "MAE: 0.1378208837349675\n",
      "MDA: 85.85858585858585%\n",
      "    ETH_Volume  BTC_Price  BTC_Volume  LINK_number_of_transfers  \\\n",
      "0          0.0   0.318740        -0.0                      -0.0   \n",
      "1          0.0   0.319027        -0.0                      -0.0   \n",
      "2          0.0   0.319047        -0.0                      -0.0   \n",
      "3          0.0   0.318196        -0.0                      -0.0   \n",
      "4          0.0   0.320140        -0.0                      -0.0   \n",
      "..         ...        ...         ...                       ...   \n",
      "95         0.0   0.322117        -0.0                      -0.0   \n",
      "96         0.0   0.322886        -0.0                      -0.0   \n",
      "97         0.0   0.323638        -0.0                      -0.0   \n",
      "98         0.0   0.323650        -0.0                      -0.0   \n",
      "99         0.0   0.322798        -0.0                      -0.0   \n",
      "\n",
      "    LINK_on_chain_volume  LINK_Price  LINK_Volume  \\\n",
      "0                   -0.0    0.256683         -0.0   \n",
      "1                   -0.0    0.256473         -0.0   \n",
      "2                   -0.0    0.256477         -0.0   \n",
      "3                   -0.0    0.256466         -0.0   \n",
      "4                   -0.0    0.254289         -0.0   \n",
      "..                   ...         ...          ...   \n",
      "95                  -0.0    0.240265         -0.0   \n",
      "96                  -0.0    0.240380         -0.0   \n",
      "97                  -0.0    0.240319         -0.0   \n",
      "98                  -0.0    0.240330         -0.0   \n",
      "99                  -0.0    0.240638         -0.0   \n",
      "\n",
      "    GWEI_average_daily_gas_price_gwei  WBTC_number_of_transfers  \\\n",
      "0                                 0.0                      -0.0   \n",
      "1                                 0.0                      -0.0   \n",
      "2                                 0.0                      -0.0   \n",
      "3                                 0.0                      -0.0   \n",
      "4                                 0.0                      -0.0   \n",
      "..                                ...                       ...   \n",
      "95                                0.0                      -0.0   \n",
      "96                                0.0                      -0.0   \n",
      "97                                0.0                      -0.0   \n",
      "98                                0.0                      -0.0   \n",
      "99                                0.0                      -0.0   \n",
      "\n",
      "    WBTC_on_chain_volume  ...  DOGE_Close_lag5  DOGE_Close_lag6  \\\n",
      "0                   -0.0  ...              0.0              0.0   \n",
      "1                   -0.0  ...              0.0              0.0   \n",
      "2                   -0.0  ...              0.0              0.0   \n",
      "3                   -0.0  ...              0.0              0.0   \n",
      "4                   -0.0  ...              0.0              0.0   \n",
      "..                   ...  ...              ...              ...   \n",
      "95                  -0.0  ...              0.0              0.0   \n",
      "96                  -0.0  ...              0.0              0.0   \n",
      "97                  -0.0  ...              0.0              0.0   \n",
      "98                  -0.0  ...              0.0              0.0   \n",
      "99                  -0.0  ...              0.0              0.0   \n",
      "\n",
      "    DOGE_Close_lag7  DOGE_Volume_lag1  DOGE_Volume_lag2  DOGE_Volume_lag3  \\\n",
      "0               0.0               0.0               0.0              -0.0   \n",
      "1               0.0               0.0               0.0              -0.0   \n",
      "2               0.0               0.0               0.0              -0.0   \n",
      "3               0.0               0.0               0.0              -0.0   \n",
      "4               0.0               0.0               0.0              -0.0   \n",
      "..              ...               ...               ...               ...   \n",
      "95              0.0               0.0               0.0              -0.0   \n",
      "96              0.0               0.0               0.0              -0.0   \n",
      "97              0.0               0.0               0.0              -0.0   \n",
      "98              0.0               0.0               0.0              -0.0   \n",
      "99              0.0               0.0               0.0              -0.0   \n",
      "\n",
      "    DOGE_Volume_lag4  DOGE_Volume_lag5  DOGE_Volume_lag6  DOGE_Volume_lag7  \n",
      "0               -0.0               0.0               0.0               0.0  \n",
      "1               -0.0               0.0               0.0               0.0  \n",
      "2               -0.0               0.0               0.0               0.0  \n",
      "3               -0.0               0.0               0.0               0.0  \n",
      "4               -0.0               0.0               0.0               0.0  \n",
      "..               ...               ...               ...               ...  \n",
      "95              -0.0               0.0               0.0               0.0  \n",
      "96              -0.0               0.0               0.0               0.0  \n",
      "97              -0.0               0.0               0.0               0.0  \n",
      "98              -0.0               0.0               0.0               0.0  \n",
      "99              -0.0               0.0               0.0               0.0  \n",
      "\n",
      "[100 rows x 496 columns]\n"
     ]
    }
   ],
   "source": [
    "n = len(df_lagged)\n",
    "\n",
    "# Doing the prediction with all the lags, LASSO\n",
    "#basically none of the lags were effective at all, so will delete this from the paper\n",
    "start = n - 100\n",
    "predictions = []\n",
    "coefficients = []\n",
    "\n",
    "# Recursive window prediction\n",
    "for i in range(start, n):\n",
    "    # Define the training data\n",
    "    X_train = X[:i]\n",
    "    y_train = Y[:i]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    # Define Lasso with 5-fold CV\n",
    "    model = LassoCV(cv=tscv, random_state=0)  # normalize parameter is deprecated\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save coefficients\n",
    "    coefficients.append(model.coef_)\n",
    "    \n",
    "    # Make prediction\n",
    "    # Use double brackets to maintain the 2D structure expected by the predict method\n",
    "    y_pred = model.predict(X[[i]])\n",
    "    predictions.append(y_pred[0])\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(Y[start:], predictions))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "actuals = Y[start:]\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Calculate MDA\n",
    "actuals_array = np.array(actuals)\n",
    "predictionsLasso_array = np.array(predictions)\n",
    "correct_direction = np.sign(actuals_array[1:] - actuals_array[:-1]) == np.sign(predictionsLasso_array[1:] - predictionsLasso_array[:-1])\n",
    "mda = np.mean(correct_direction) * 100\n",
    "print(f\"MDA: {mda}%\")\n",
    "\n",
    "# Convert coefficients to a DataFrame for better visualization\n",
    "# Use the correct column names from the lagged DataFrame\n",
    "coef_df = pd.DataFrame(coefficients, columns=df_lagged.columns)\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "528f92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_coef = coef_df.iloc[-1]\n",
    "\n",
    "# Get the sorted column names based on the absolute values of the coefficients from the last row\n",
    "sorted_columns = last_coef.abs().sort_values(ascending=False).index\n",
    "\n",
    "# Reorder the entire DataFrame based on these sorted column names\n",
    "sorted_coef_df = coef_df[sorted_columns]\n",
    "\n",
    "# Export to CSV\n",
    "sorted_coef_df.to_csv('/Users/gadimg/Library/Mobile Documents/com~apple~CloudDocs/PhD dis/third/Results/pct_change_results/Lasso_lags.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ce98858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR Model RMSE for the last 100 values: 0.37212472328292573\n",
      "AR Model MAE for the last 100 values: 0.25699656359109113\n",
      "AR Model MDA for the last 100 values: 66.66666666666666%\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "n = len(df_normalized)\n",
    "# Your Y and start are already defined from the Lasso part\n",
    "# Recursive window prediction for AR model, straight up with 7 lags\n",
    "start = n - 100\n",
    "\n",
    "ar_predictions = []\n",
    "\n",
    "for i in range(start, n):\n",
    "    # Define the training data up to point i\n",
    "    y_train_ar = Y[:i]\n",
    "    \n",
    "    # Fit the AR model - we'll use a fixed lag of 7, you might have to adjust based on your previous criteria\n",
    "    ar_model = AutoReg(y_train_ar, lags=1, old_names=False).fit()\n",
    "    \n",
    "    # Make the one-step-ahead forecast\n",
    "    y_pred_ar = ar_model.predict(start=i, end=i)\n",
    "    ar_predictions.append(y_pred_ar[0])\n",
    "\n",
    "# Calculate RMSE for the AR predictions\n",
    "ar_rmse = np.sqrt(mean_squared_error(Y[start:], ar_predictions))\n",
    "print(f\"AR Model RMSE for the last 100 values: {ar_rmse}\")\n",
    "\n",
    "# Calculate MAE for the AR predictions\n",
    "ar_mae = mean_absolute_error(Y[start:], ar_predictions)\n",
    "print(f\"AR Model MAE for the last 100 values: {ar_mae}\")\n",
    "\n",
    "# Calculate MDA for the AR predictions\n",
    "actuals_array = np.array(Y[start:])\n",
    "predictions_array = np.array(ar_predictions)\n",
    "\n",
    "# Calculate the direction of change for actuals and predictions\n",
    "actual_direction = np.sign(actuals_array[1:] - actuals_array[:-1])\n",
    "predicted_direction = np.sign(predictions_array[1:] - predictions_array[:-1])\n",
    "\n",
    "# Calculate Mean Directional Accuracy\n",
    "correct_direction = actual_direction == predicted_direction\n",
    "ar_mda = np.mean(correct_direction) * 100\n",
    "print(f\"AR Model MDA for the last 100 values: {ar_mda}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aeaae29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The series is stationary.\n",
      "Optimal number of lags based on full data AIC: 7\n"
     ]
    }
   ],
   "source": [
    "#determining the appropriate lags \n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Make sure your series is stationary\n",
    "result = adfuller(Y)\n",
    "if result[1] < 0.05:\n",
    "    print(\"The series is stationary.\")\n",
    "    optimal_lag = None\n",
    "    best_aic = np.inf\n",
    "    \n",
    "    # Determine the optimal number of lags using AIC on the full dataset\n",
    "    for lag in range(1, 8):  # You can adjust the range of lags you want to test\n",
    "        model = AutoReg(Y, lags=lag, old_names=False).fit()\n",
    "        if model.aic < best_aic:\n",
    "            best_aic = model.aic\n",
    "            optimal_lag = lag\n",
    "\n",
    "    print(f\"Optimal number of lags based on full data AIC: {optimal_lag}\")\n",
    "else:\n",
    "    print(\"The series is not stationary. Make it stationary before applying AR model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fee5acde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationarity test for ETH_Price\n",
      "ADF Statistic: -10.287806261215838\n",
      "p-value: 3.636009721221062e-18\n",
      "Critical Value (1%): -3.436039176245612\n",
      "Critical Value (5%): -2.8640521575678655\n",
      "Critical Value (10%): -2.5681075333056036\n",
      "\n",
      "Stationarity test for ETH_Volume\n",
      "ADF Statistic: -6.220921577286217\n",
      "p-value: 5.225937108414839e-08\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for BTC_Price\n",
      "ADF Statistic: -34.70175148689677\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for BTC_Volume\n",
      "ADF Statistic: -6.507441088827191\n",
      "p-value: 1.1219897785585668e-08\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for LINK_number_of_transfers\n",
      "ADF Statistic: -4.717061514675348\n",
      "p-value: 7.815689924464144e-05\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for LINK_on_chain_volume\n",
      "ADF Statistic: -27.316810423690114\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.435999853208029\n",
      "Critical Value (5%): -2.864034810134692\n",
      "Critical Value (10%): -2.5680982944411417\n",
      "\n",
      "Stationarity test for LINK_Price\n",
      "ADF Statistic: -17.44388175988007\n",
      "p-value: 4.693006564305852e-30\n",
      "Critical Value (1%): -3.436014535478888\n",
      "Critical Value (5%): -2.8640412872730208\n",
      "Critical Value (10%): -2.56810174401953\n",
      "\n",
      "Stationarity test for LINK_Volume\n",
      "ADF Statistic: -34.81113532190825\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for GWEI_average_daily_gas_price_gwei\n",
      "ADF Statistic: -35.94071160596955\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for WBTC_number_of_transfers\n",
      "ADF Statistic: -17.643481141161217\n",
      "p-value: 3.749278808912833e-30\n",
      "Critical Value (1%): -3.4360194465416387\n",
      "Critical Value (5%): -2.8640434537995523\n",
      "Critical Value (10%): -2.5681028978640104\n",
      "\n",
      "Stationarity test for WBTC_on_chain_volume\n",
      "ADF Statistic: -6.940876575980243\n",
      "p-value: 1.0258955320730905e-09\n",
      "Critical Value (1%): -3.4360590437486405\n",
      "Critical Value (5%): -2.864060922068716\n",
      "Critical Value (10%): -2.5681122011011426\n",
      "\n",
      "Stationarity test for WBTC_Price\n",
      "ADF Statistic: -34.89603085755851\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for WBTC_Volume\n",
      "ADF Statistic: -6.741053417132851\n",
      "p-value: 3.11816328450765e-09\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for USDT_number_of_transfers\n",
      "ADF Statistic: -6.00279503082805\n",
      "p-value: 1.6421156647941038e-07\n",
      "Critical Value (1%): -3.4361042569951805\n",
      "Critical Value (5%): -2.8640808675675826\n",
      "Critical Value (10%): -2.568122823703617\n",
      "\n",
      "Stationarity test for USDT_on_chain_volume\n",
      "ADF Statistic: -6.714035292295185\n",
      "p-value: 3.6198546822390508e-09\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for USDT_Price\n",
      "ADF Statistic: -11.209880753805116\n",
      "p-value: 2.1386480362433558e-20\n",
      "Critical Value (1%): -3.4360941478268767\n",
      "Critical Value (5%): -2.864076408006588\n",
      "Critical Value (10%): -2.568120448620112\n",
      "\n",
      "Stationarity test for USDT_Volume\n",
      "ADF Statistic: -6.831477932133326\n",
      "p-value: 1.8888236506281812e-09\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for USDC_number_of_transfers\n",
      "ADF Statistic: -8.004354407462209\n",
      "p-value: 2.2888248538586658e-12\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for USDC_on_chain_volume\n",
      "ADF Statistic: -7.097601299628679\n",
      "p-value: 4.2492373169547105e-10\n",
      "Critical Value (1%): -3.4360941478268767\n",
      "Critical Value (5%): -2.864076408006588\n",
      "Critical Value (10%): -2.568120448620112\n",
      "\n",
      "Stationarity test for USDC_Price\n",
      "ADF Statistic: -14.350681336339289\n",
      "p-value: 1.0229858830838035e-26\n",
      "Critical Value (1%): -3.436064032324827\n",
      "Critical Value (5%): -2.864063122757945\n",
      "Critical Value (10%): -2.5681133731450605\n",
      "\n",
      "Stationarity test for USDC_Volume\n",
      "ADF Statistic: -6.692600349199117\n",
      "p-value: 4.073879314048971e-09\n",
      "Critical Value (1%): -3.4360941478268767\n",
      "Critical Value (5%): -2.864076408006588\n",
      "Critical Value (10%): -2.568120448620112\n",
      "\n",
      "Stationarity test for MATIC_number_of_transfers\n",
      "ADF Statistic: -20.376224261861395\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.436009632917059\n",
      "Critical Value (5%): -2.864039124493168\n",
      "Critical Value (10%): -2.5681005921710054\n",
      "\n",
      "Stationarity test for MATIC_on_chain_volume\n",
      "ADF Statistic: -35.53240582503092\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for MATIC_Price\n",
      "ADF Statistic: -4.903909490392538\n",
      "p-value: 3.426840818382748e-05\n",
      "Critical Value (1%): -3.4360941478268767\n",
      "Critical Value (5%): -2.864076408006588\n",
      "Critical Value (10%): -2.568120448620112\n",
      "\n",
      "Stationarity test for MATIC_Volume\n",
      "ADF Statistic: -6.251795261580803\n",
      "p-value: 4.435641364266756e-08\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for SHIB_number_of_transfers\n",
      "ADF Statistic: -33.91700989383468\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for SHIB_on_chain_volume\n",
      "ADF Statistic: -34.07345038003347\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for SHIB_Price\n",
      "ADF Statistic: -4.467679970242962\n",
      "p-value: 0.000224842626613674\n",
      "Critical Value (1%): -3.4361093249345402\n",
      "Critical Value (5%): -2.8640831032339706\n",
      "Critical Value (10%): -2.5681240143809787\n",
      "\n",
      "Stationarity test for SHIB_Volume\n",
      "ADF Statistic: -5.693096084749541\n",
      "p-value: 7.987315810768572e-07\n",
      "Critical Value (1%): -3.4360690296267338\n",
      "Critical Value (5%): -2.864065327292836\n",
      "Critical Value (10%): -2.568114547237679\n",
      "\n",
      "Stationarity test for WETH_number_of_transfers\n",
      "ADF Statistic: -11.52327410842487\n",
      "p-value: 3.992079291279793e-21\n",
      "Critical Value (1%): -3.4360590437486405\n",
      "Critical Value (5%): -2.864060922068716\n",
      "Critical Value (10%): -2.5681122011011426\n",
      "\n",
      "Stationarity test for WETH_on_chain_volume\n",
      "ADF Statistic: -24.42946056393239\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.4360047388340984\n",
      "Critical Value (5%): -2.8640369654502846\n",
      "Critical Value (10%): -2.5680994423132613\n",
      "\n",
      "Stationarity test for WETH_Volume\n",
      "ADF Statistic: -34.21525047008819\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for DAI_number_of_transfers\n",
      "ADF Statistic: -12.561481752482482\n",
      "p-value: 2.0978712230938558e-23\n",
      "Critical Value (1%): -3.4360490926821727\n",
      "Critical Value (5%): -2.8640565321870635\n",
      "Critical Value (10%): -2.5681098631379964\n",
      "\n",
      "Stationarity test for DAI_on_chain_volume\n",
      "ADF Statistic: -37.134308679125304\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for DAI_Price\n",
      "ADF Statistic: -13.787348857638106\n",
      "p-value: 9.060121102761964e-26\n",
      "Critical Value (1%): -3.4361093249345402\n",
      "Critical Value (5%): -2.8640831032339706\n",
      "Critical Value (10%): -2.5681240143809787\n",
      "\n",
      "Stationarity test for DAI_Volume\n",
      "ADF Statistic: -9.895580730558386\n",
      "p-value: 3.465142987514942e-17\n",
      "Critical Value (1%): -3.4360590437486405\n",
      "Critical Value (5%): -2.864060922068716\n",
      "Critical Value (10%): -2.5681122011011426\n",
      "\n",
      "Stationarity test for new_tokens_monthly_x\n",
      "ADF Statistic: -34.10152821292108\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for CPILFESL\n",
      "ADF Statistic: -11.406454610064808\n",
      "p-value: 7.429144629391796e-21\n",
      "Critical Value (1%): -3.4361093249345402\n",
      "Critical Value (5%): -2.8640831032339706\n",
      "Critical Value (10%): -2.5681240143809787\n",
      "\n",
      "Stationarity test for EUEPUINDXM\n",
      "ADF Statistic: -34.062759513500694\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for CHNMAINLANDEPU\n",
      "ADF Statistic: -34.053250777648856\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for NASDAQ_Close\n",
      "ADF Statistic: -9.461632629592751\n",
      "p-value: 4.33809824481667e-16\n",
      "Critical Value (1%): -3.4360590437486405\n",
      "Critical Value (5%): -2.864060922068716\n",
      "Critical Value (10%): -2.5681122011011426\n",
      "\n",
      "Stationarity test for NASDAQ_Volume\n",
      "ADF Statistic: -12.667188899236418\n",
      "p-value: 1.2672938950661959e-23\n",
      "Critical Value (1%): -3.4360590437486405\n",
      "Critical Value (5%): -2.864060922068716\n",
      "Critical Value (10%): -2.5681122011011426\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationarity test for TNX_Close\n",
      "ADF Statistic: -16.173308251140902\n",
      "p-value: 4.342009531980949e-29\n",
      "Critical Value (1%): -3.4360194465416387\n",
      "Critical Value (5%): -2.8640434537995523\n",
      "Critical Value (10%): -2.5681028978640104\n",
      "\n",
      "Stationarity test for USEPUINDXD\n",
      "ADF Statistic: -4.8945915240830065\n",
      "p-value: 3.5729665826600136e-05\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for FFR\n",
      "ADF Statistic: -34.78066695634193\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for NIKKEI225\n",
      "ADF Statistic: -22.531252416876296\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.435999853208029\n",
      "Critical Value (5%): -2.864034810134692\n",
      "Critical Value (10%): -2.5680982944411417\n",
      "\n",
      "Stationarity test for DCOILBRENTEU\n",
      "ADF Statistic: -8.850925166688606\n",
      "p-value: 1.5737745716419707e-14\n",
      "Critical Value (1%): -3.4360690296267338\n",
      "Critical Value (5%): -2.864065327292836\n",
      "Critical Value (10%): -2.568114547237679\n",
      "\n",
      "Stationarity test for SP500\n",
      "ADF Statistic: -9.476977164900381\n",
      "p-value: 3.965433588020087e-16\n",
      "Critical Value (1%): -3.4360590437486405\n",
      "Critical Value (5%): -2.864060922068716\n",
      "Critical Value (10%): -2.5681122011011426\n",
      "\n",
      "Stationarity test for GOOGTREND\n",
      "ADF Statistic: -6.411683429892215\n",
      "p-value: 1.88402576310145e-08\n",
      "Critical Value (1%): -3.4360941478268767\n",
      "Critical Value (5%): -2.864076408006588\n",
      "Critical Value (10%): -2.568120448620112\n",
      "\n",
      "Stationarity test for TRX_Close\n",
      "ADF Statistic: -8.656743272371774\n",
      "p-value: 4.945486981078483e-14\n",
      "Critical Value (1%): -3.4360840741163168\n",
      "Critical Value (5%): -2.864071964072552\n",
      "Critical Value (10%): -2.568118081861596\n",
      "\n",
      "Stationarity test for TRX_Volume\n",
      "ADF Statistic: -29.12624403570698\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.435999853208029\n",
      "Critical Value (5%): -2.864034810134692\n",
      "Critical Value (10%): -2.5680982944411417\n",
      "\n",
      "Stationarity test for XRP_Close\n",
      "ADF Statistic: -35.2243197559134\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for XRP_Volume\n",
      "ADF Statistic: -7.328687235800898\n",
      "p-value: 1.1429557168796705e-10\n",
      "Critical Value (1%): -3.4360991979671205\n",
      "Critical Value (5%): -2.864078635828568\n",
      "Critical Value (10%): -2.568121635118498\n",
      "\n",
      "Stationarity test for DOT_Close\n",
      "ADF Statistic: -11.610833830075668\n",
      "p-value: 2.5155268483605392e-21\n",
      "Critical Value (1%): -3.4360342309571053\n",
      "Critical Value (5%): -2.864049975956835\n",
      "Critical Value (10%): -2.568106371425197\n",
      "\n",
      "Stationarity test for DOT_Volume\n",
      "ADF Statistic: -10.896861736477591\n",
      "p-value: 1.1861714987430752e-19\n",
      "Critical Value (1%): -3.436064032324827\n",
      "Critical Value (5%): -2.864063122757945\n",
      "Critical Value (10%): -2.5681133731450605\n",
      "\n",
      "Stationarity test for ADA_Close\n",
      "ADF Statistic: -9.998670860030547\n",
      "p-value: 1.9099056988346033e-17\n",
      "Critical Value (1%): -3.4360342309571053\n",
      "Critical Value (5%): -2.864049975956835\n",
      "Critical Value (10%): -2.568106371425197\n",
      "\n",
      "Stationarity test for ADA_Volume\n",
      "ADF Statistic: -9.783990906331042\n",
      "p-value: 6.618025336087426e-17\n",
      "Critical Value (1%): -3.436089106551049\n",
      "Critical Value (5%): -2.864074184091338\n",
      "Critical Value (10%): -2.568119264202968\n",
      "\n",
      "Stationarity test for BNB_Close\n",
      "ADF Statistic: -7.963937263414021\n",
      "p-value: 2.898687982992021e-12\n",
      "Critical Value (1%): -3.436039176245612\n",
      "Critical Value (5%): -2.8640521575678655\n",
      "Critical Value (10%): -2.5681075333056036\n",
      "\n",
      "Stationarity test for BNB_Volume\n",
      "ADF Statistic: -9.112838282122205\n",
      "p-value: 3.363251014282244e-15\n",
      "Critical Value (1%): -3.436064032324827\n",
      "Critical Value (5%): -2.864063122757945\n",
      "Critical Value (10%): -2.5681133731450605\n",
      "\n",
      "Stationarity test for SOL_Close\n",
      "ADF Statistic: -35.47640147125354\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n",
      "Stationarity test for SOL_Volume\n",
      "ADF Statistic: -8.716668949471996\n",
      "p-value: 3.473388684183641e-14\n",
      "Critical Value (1%): -3.436064032324827\n",
      "Critical Value (5%): -2.864063122757945\n",
      "Critical Value (10%): -2.5681133731450605\n",
      "\n",
      "Stationarity test for DOGE_Close\n",
      "ADF Statistic: -15.627551231642725\n",
      "p-value: 1.7013480695409888e-28\n",
      "Critical Value (1%): -3.436009632917059\n",
      "Critical Value (5%): -2.864039124493168\n",
      "Critical Value (10%): -2.5681005921710054\n",
      "\n",
      "Stationarity test for DOGE_Volume\n",
      "ADF Statistic: -34.58506033434057\n",
      "p-value: 0.0\n",
      "Critical Value (1%): -3.43599497601695\n",
      "Critical Value (5%): -2.8640326585367473\n",
      "Critical Value (10%): -2.5680971485495068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testiing for stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series, column_name):\n",
    "    result = adfuller(series, autolag='AIC')\n",
    "    print(f'Stationarity test for {column_name}')\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'Critical Value ({key}): {value}')\n",
    "    print('')\n",
    "\n",
    "# Test for stationarity on the original dataframe before normalization\n",
    "#for column in df.columns:\n",
    "    #if column != 'Date':  # Skip the 'Date' column\n",
    "     #   adf_test(df[column], column)\n",
    "\n",
    "# Test for stationarity on the normalized dataframe\n",
    "for column in df_normalized.columns:\n",
    "    adf_test(df_normalized[column], column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d56c8ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016387045098539943, tolerance: 0.015053884194834118\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022948103015558274, tolerance: 0.02178182265957419\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017934149755120643, tolerance: 0.01676022608955386\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.026560939782314108, tolerance: 0.02629731475134505\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022885258666249797, tolerance: 0.02275691992522633\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03221906522096418, tolerance: 0.02501672472013743\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04921253179704976, tolerance: 0.019997492790087588\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016761717648648755, tolerance: 0.015714482211118323\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02107508190442786, tolerance: 0.015714482211118323\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02085539399008951, tolerance: 0.015714482211118323\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017725952693641744, tolerance: 0.015714482211118323\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015764858002851767, tolerance: 0.015714482211118323\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03148842627648918, tolerance: 0.028275346240896955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.031537083149004275, tolerance: 0.028275346240896955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030664847344731072, tolerance: 0.028275346240896955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029538312507241926, tolerance: 0.028275346240896955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02834250024067586, tolerance: 0.028275346240896955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029144005850298527, tolerance: 0.028275346240896955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02907597041349419, tolerance: 0.028275346240896955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.028275763316962355, tolerance: 0.028275346240896955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0327838352860681, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03303168451074612, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03234288053754142, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03147419738835566, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030514382578161303, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029384788995471922, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.028341957005309837, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.027977512862221943, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030263258871649157, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030533391645796826, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029548059901230772, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02860258914481051, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.027582035047100106, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02657315776825442, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02548191236772368, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.025126321014496966, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.024019055626279595, tolerance: 0.023188601625220403\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015757133948106627, tolerance: 0.01447640161717305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01753268814962894, tolerance: 0.01447640161717305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017331684909308365, tolerance: 0.01447640161717305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016932476008065578, tolerance: 0.01447640161717305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016466619238006075, tolerance: 0.01447640161717305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015956408995585036, tolerance: 0.01447640161717305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017079078951312, tolerance: 0.016263453519969984\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0273462529526185, tolerance: 0.016263453519969984\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030320512375737962, tolerance: 0.016263453519969984\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030439856954490807, tolerance: 0.016263453519969984\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01655699158520818, tolerance: 0.016470598205172236\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017010216907387132, tolerance: 0.016470598205172236\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017806732930893077, tolerance: 0.016470598205172236\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017824811700709375, tolerance: 0.016470598205172236\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017418561724866777, tolerance: 0.016470598205172236\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03837582693402197, tolerance: 0.018412760583105873\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20007615145002688, tolerance: 0.018412760583105873\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26524062658610603, tolerance: 0.018412760583105873\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.023148013800440737, tolerance: 0.022099422666630595\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022588465742266806, tolerance: 0.022099422666630595\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020860732093495926, tolerance: 0.01923770675655727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022518036597730884, tolerance: 0.016212124835406752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.041984472762806035, tolerance: 0.016212124835406752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.046403842427483966, tolerance: 0.016212124835406752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.036997483657579266, tolerance: 0.018722818997918528\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05052064884260332, tolerance: 0.018722818997918528\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05296629491471805, tolerance: 0.018722818997918528\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.052241152726963946, tolerance: 0.018722818997918528\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02958271578815186, tolerance: 0.02587589837724752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.031378024480723354, tolerance: 0.02587589837724752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0307163233436043, tolerance: 0.02587589837724752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029590968997489142, tolerance: 0.02587589837724752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.028404335374617062, tolerance: 0.02587589837724752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.027211213350916807, tolerance: 0.02587589837724752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.026026818763082105, tolerance: 0.02587589837724752\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.018612840295240574, tolerance: 0.013715401116942662\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.019275534587180232, tolerance: 0.019240591974912808\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12090274228813058, tolerance: 0.019240591974912808\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022139333190629884, tolerance: 0.016920944479131343\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02204012785804821, tolerance: 0.016920944479131343\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02134766004062172, tolerance: 0.016920944479131343\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029848198145746352, tolerance: 0.025082068609092546\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.027583254463401374, tolerance: 0.025082068609092546\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.026005430966790755, tolerance: 0.025082068609092546\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.023082811684218996, tolerance: 0.01813327523108269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02306613589345119, tolerance: 0.01813327523108269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022380088422742972, tolerance: 0.01813327523108269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02147479288635168, tolerance: 0.01813327523108269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020560818489421706, tolerance: 0.01813327523108269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.019664102467718436, tolerance: 0.01813327523108269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04683083845981173, tolerance: 0.02499495554289541\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04114947616440645, tolerance: 0.02499495554289541\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04026908197639756, tolerance: 0.02499495554289541\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04065742084726054, tolerance: 0.02499495554289541\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0396427598286877, tolerance: 0.02499495554289541\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.038401130978325426, tolerance: 0.02499495554289541\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03219859829760452, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03263106429391627, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.036783773173830525, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0370734881104795, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.036811417096892285, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.036076641696148215, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.035059037470524856, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03398879049990455, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03283741342144708, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03149577746912868, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03194315205258391, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03153918955788271, tolerance: 0.030485866227712148\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03702867661027298, tolerance: 0.020680437497142275\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.025523415450926734, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06145883457590884, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0714189239755676, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07311074593965827, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07210321996218028, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05863148631866011, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.053112617293777475, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05091353353665795, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04946241350772951, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.042561560696963596, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04186769724011796, tolerance: 0.023397962213424543\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0584644690256404, tolerance: 0.019381063339617432\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07657264457259316, tolerance: 0.019381063339617432\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08221072155019726, tolerance: 0.019381063339617432\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05893663032305341, tolerance: 0.019501352103941223\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1409313793524376, tolerance: 0.019501352103941223\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030035829781468237, tolerance: 0.02407107568947175\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.039632863068675306, tolerance: 0.02407107568947175\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03347658835470213, tolerance: 0.027710866281677706\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03349310125427252, tolerance: 0.027710866281677706\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03247378887272134, tolerance: 0.027710866281677706\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03130384321938351, tolerance: 0.027710866281677706\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03014299331626802, tolerance: 0.027710866281677706\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02969142218929477, tolerance: 0.027710866281677706\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.031134178292369086, tolerance: 0.027710866281677706\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/gadimg/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05462791249671284, tolerance: 0.02228819454370708\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: ETH_Volume\n",
      "Zero Proportion: 0.029\n",
      "95% Confidence Interval: (0.00000, 0.33937)\n",
      "The confidence interval for ETH_Volume includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: BTC_Price\n",
      "Zero Proportion: 0.025\n",
      "95% Confidence Interval: (0.00169, 0.40475)\n",
      "The confidence interval for BTC_Price does not include zero, suggesting a significant effect.\n",
      "\n",
      "\n",
      "Feature: BTC_Volume\n",
      "Zero Proportion: 0.069\n",
      "95% Confidence Interval: (-0.17972, 0.00000)\n",
      "The confidence interval for BTC_Volume includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: LINK_number_of_transfers\n",
      "Zero Proportion: 0.805\n",
      "95% Confidence Interval: (-0.00707, 0.00797)\n",
      "The confidence interval for LINK_number_of_transfers includes zero, suggesting no significant effect.\n",
      "The feature LINK_number_of_transfers was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: LINK_on_chain_volume\n",
      "Zero Proportion: 0.852\n",
      "95% Confidence Interval: (-0.00493, 0.00581)\n",
      "The confidence interval for LINK_on_chain_volume includes zero, suggesting no significant effect.\n",
      "The feature LINK_on_chain_volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: LINK_Price\n",
      "Zero Proportion: 0.000\n",
      "95% Confidence Interval: (0.15666, 0.27313)\n",
      "The confidence interval for LINK_Price does not include zero, suggesting a significant effect.\n",
      "\n",
      "\n",
      "Feature: LINK_Volume\n",
      "Zero Proportion: 0.766\n",
      "95% Confidence Interval: (0.00000, 0.03795)\n",
      "The confidence interval for LINK_Volume includes zero, suggesting no significant effect.\n",
      "The feature LINK_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: GWEI_average_daily_gas_price_gwei\n",
      "Zero Proportion: 0.510\n",
      "95% Confidence Interval: (-0.00729, 0.02314)\n",
      "The confidence interval for GWEI_average_daily_gas_price_gwei includes zero, suggesting no significant effect.\n",
      "The feature GWEI_average_daily_gas_price_gwei was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: WBTC_number_of_transfers\n",
      "Zero Proportion: 0.298\n",
      "95% Confidence Interval: (-0.06620, 0.00000)\n",
      "The confidence interval for WBTC_number_of_transfers includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: WBTC_on_chain_volume\n",
      "Zero Proportion: 0.824\n",
      "95% Confidence Interval: (0.00000, 0.01071)\n",
      "The confidence interval for WBTC_on_chain_volume includes zero, suggesting no significant effect.\n",
      "The feature WBTC_on_chain_volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: WBTC_Price\n",
      "Zero Proportion: 0.365\n",
      "95% Confidence Interval: (0.00000, 0.34269)\n",
      "The confidence interval for WBTC_Price includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: WBTC_Volume\n",
      "Zero Proportion: 0.475\n",
      "95% Confidence Interval: (0.00000, 0.05790)\n",
      "The confidence interval for WBTC_Volume includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: USDT_number_of_transfers\n",
      "Zero Proportion: 0.851\n",
      "95% Confidence Interval: (-0.00276, 0.01247)\n",
      "The confidence interval for USDT_number_of_transfers includes zero, suggesting no significant effect.\n",
      "The feature USDT_number_of_transfers was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: USDT_on_chain_volume\n",
      "Zero Proportion: 0.863\n",
      "95% Confidence Interval: (-0.00881, 0.01417)\n",
      "The confidence interval for USDT_on_chain_volume includes zero, suggesting no significant effect.\n",
      "The feature USDT_on_chain_volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: USDT_Price\n",
      "Zero Proportion: 0.684\n",
      "95% Confidence Interval: (-0.01098, 0.02009)\n",
      "The confidence interval for USDT_Price includes zero, suggesting no significant effect.\n",
      "The feature USDT_Price was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: USDT_Volume\n",
      "Zero Proportion: 0.947\n",
      "95% Confidence Interval: (-0.01176, 0.00000)\n",
      "The confidence interval for USDT_Volume includes zero, suggesting no significant effect.\n",
      "The feature USDT_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: USDC_number_of_transfers\n",
      "Zero Proportion: 0.650\n",
      "95% Confidence Interval: (-0.03105, 0.00000)\n",
      "The confidence interval for USDC_number_of_transfers includes zero, suggesting no significant effect.\n",
      "The feature USDC_number_of_transfers was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: USDC_on_chain_volume\n",
      "Zero Proportion: 0.610\n",
      "95% Confidence Interval: (-0.01886, 0.00000)\n",
      "The confidence interval for USDC_on_chain_volume includes zero, suggesting no significant effect.\n",
      "The feature USDC_on_chain_volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: USDC_Price\n",
      "Zero Proportion: 0.278\n",
      "95% Confidence Interval: (-0.04696, 0.00000)\n",
      "The confidence interval for USDC_Price includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: USDC_Volume\n",
      "Zero Proportion: 0.819\n",
      "95% Confidence Interval: (0.00000, 0.00441)\n",
      "The confidence interval for USDC_Volume includes zero, suggesting no significant effect.\n",
      "The feature USDC_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: MATIC_number_of_transfers\n",
      "Zero Proportion: 0.732\n",
      "95% Confidence Interval: (-0.01398, 0.01767)\n",
      "The confidence interval for MATIC_number_of_transfers includes zero, suggesting no significant effect.\n",
      "The feature MATIC_number_of_transfers was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: MATIC_on_chain_volume\n",
      "Zero Proportion: 0.934\n",
      "95% Confidence Interval: (-0.00223, 0.00000)\n",
      "The confidence interval for MATIC_on_chain_volume includes zero, suggesting no significant effect.\n",
      "The feature MATIC_on_chain_volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: MATIC_Price\n",
      "Zero Proportion: 0.003\n",
      "95% Confidence Interval: (0.02324, 0.14613)\n",
      "The confidence interval for MATIC_Price does not include zero, suggesting a significant effect.\n",
      "\n",
      "\n",
      "Feature: MATIC_Volume\n",
      "Zero Proportion: 0.383\n",
      "95% Confidence Interval: (-0.05296, 0.00000)\n",
      "The confidence interval for MATIC_Volume includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: SHIB_number_of_transfers\n",
      "Zero Proportion: 0.544\n",
      "95% Confidence Interval: (-0.01758, 0.00000)\n",
      "The confidence interval for SHIB_number_of_transfers includes zero, suggesting no significant effect.\n",
      "The feature SHIB_number_of_transfers was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: SHIB_on_chain_volume\n",
      "Zero Proportion: 0.732\n",
      "95% Confidence Interval: (0.00000, 0.00764)\n",
      "The confidence interval for SHIB_on_chain_volume includes zero, suggesting no significant effect.\n",
      "The feature SHIB_on_chain_volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: SHIB_Price\n",
      "Zero Proportion: 0.619\n",
      "95% Confidence Interval: (-0.01797, 0.00098)\n",
      "The confidence interval for SHIB_Price includes zero, suggesting no significant effect.\n",
      "The feature SHIB_Price was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: SHIB_Volume\n",
      "Zero Proportion: 0.748\n",
      "95% Confidence Interval: (-0.01277, 0.00000)\n",
      "The confidence interval for SHIB_Volume includes zero, suggesting no significant effect.\n",
      "The feature SHIB_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: WETH_number_of_transfers\n",
      "Zero Proportion: 0.593\n",
      "95% Confidence Interval: (-0.00641, 0.02964)\n",
      "The confidence interval for WETH_number_of_transfers includes zero, suggesting no significant effect.\n",
      "The feature WETH_number_of_transfers was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: WETH_on_chain_volume\n",
      "Zero Proportion: 0.511\n",
      "95% Confidence Interval: (-0.03620, 0.00000)\n",
      "The confidence interval for WETH_on_chain_volume includes zero, suggesting no significant effect.\n",
      "The feature WETH_on_chain_volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: WETH_Volume\n",
      "Zero Proportion: 0.773\n",
      "95% Confidence Interval: (-0.00638, 0.00000)\n",
      "The confidence interval for WETH_Volume includes zero, suggesting no significant effect.\n",
      "The feature WETH_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: DAI_number_of_transfers\n",
      "Zero Proportion: 0.236\n",
      "95% Confidence Interval: (-0.06066, 0.00000)\n",
      "The confidence interval for DAI_number_of_transfers includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: DAI_on_chain_volume\n",
      "Zero Proportion: 0.479\n",
      "95% Confidence Interval: (-0.02557, 0.00000)\n",
      "The confidence interval for DAI_on_chain_volume includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: DAI_Price\n",
      "Zero Proportion: 0.461\n",
      "95% Confidence Interval: (0.00000, 0.03687)\n",
      "The confidence interval for DAI_Price includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: DAI_Volume\n",
      "Zero Proportion: 0.797\n",
      "95% Confidence Interval: (-0.01950, 0.01090)\n",
      "The confidence interval for DAI_Volume includes zero, suggesting no significant effect.\n",
      "The feature DAI_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: new_tokens_monthly_x\n",
      "Zero Proportion: 0.333\n",
      "95% Confidence Interval: (0.00000, 0.03669)\n",
      "The confidence interval for new_tokens_monthly_x includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: CPILFESL\n",
      "Zero Proportion: 0.587\n",
      "95% Confidence Interval: (-0.00151, 0.02205)\n",
      "The confidence interval for CPILFESL includes zero, suggesting no significant effect.\n",
      "The feature CPILFESL was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: EUEPUINDXM\n",
      "Zero Proportion: 0.707\n",
      "95% Confidence Interval: (-0.01834, 0.01382)\n",
      "The confidence interval for EUEPUINDXM includes zero, suggesting no significant effect.\n",
      "The feature EUEPUINDXM was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: CHNMAINLANDEPU\n",
      "Zero Proportion: 0.619\n",
      "95% Confidence Interval: (0.00000, 0.02173)\n",
      "The confidence interval for CHNMAINLANDEPU includes zero, suggesting no significant effect.\n",
      "The feature CHNMAINLANDEPU was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: NASDAQ_Close\n",
      "Zero Proportion: 0.493\n",
      "95% Confidence Interval: (0.00000, 0.03223)\n",
      "The confidence interval for NASDAQ_Close includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: NASDAQ_Volume\n",
      "Zero Proportion: 0.736\n",
      "95% Confidence Interval: (-0.01248, 0.00733)\n",
      "The confidence interval for NASDAQ_Volume includes zero, suggesting no significant effect.\n",
      "The feature NASDAQ_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: TNX_Close\n",
      "Zero Proportion: 0.631\n",
      "95% Confidence Interval: (-0.02002, 0.00624)\n",
      "The confidence interval for TNX_Close includes zero, suggesting no significant effect.\n",
      "The feature TNX_Close was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: USEPUINDXD\n",
      "Zero Proportion: 0.703\n",
      "95% Confidence Interval: (-0.01547, 0.00455)\n",
      "The confidence interval for USEPUINDXD includes zero, suggesting no significant effect.\n",
      "The feature USEPUINDXD was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: FFR\n",
      "Zero Proportion: 0.383\n",
      "95% Confidence Interval: (0.00000, 0.02592)\n",
      "The confidence interval for FFR includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: NIKKEI225\n",
      "Zero Proportion: 0.454\n",
      "95% Confidence Interval: (0.00000, 0.02743)\n",
      "The confidence interval for NIKKEI225 includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: DCOILBRENTEU\n",
      "Zero Proportion: 0.567\n",
      "95% Confidence Interval: (-0.00095, 0.02162)\n",
      "The confidence interval for DCOILBRENTEU includes zero, suggesting no significant effect.\n",
      "The feature DCOILBRENTEU was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: SP500\n",
      "Zero Proportion: 0.810\n",
      "95% Confidence Interval: (0.00000, 0.02142)\n",
      "The confidence interval for SP500 includes zero, suggesting no significant effect.\n",
      "The feature SP500 was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: GOOGTREND\n",
      "Zero Proportion: 0.041\n",
      "95% Confidence Interval: (0.00000, 0.11640)\n",
      "The confidence interval for GOOGTREND includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: TRX_Close\n",
      "Zero Proportion: 0.085\n",
      "95% Confidence Interval: (0.00000, 0.07351)\n",
      "The confidence interval for TRX_Close includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: TRX_Volume\n",
      "Zero Proportion: 0.827\n",
      "95% Confidence Interval: (-0.01061, 0.00792)\n",
      "The confidence interval for TRX_Volume includes zero, suggesting no significant effect.\n",
      "The feature TRX_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: XRP_Close\n",
      "Zero Proportion: 0.149\n",
      "95% Confidence Interval: (0.00000, 0.06730)\n",
      "The confidence interval for XRP_Close includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: XRP_Volume\n",
      "Zero Proportion: 0.703\n",
      "95% Confidence Interval: (-0.02704, 0.00000)\n",
      "The confidence interval for XRP_Volume includes zero, suggesting no significant effect.\n",
      "The feature XRP_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: DOT_Close\n",
      "Zero Proportion: 0.000\n",
      "95% Confidence Interval: (0.04646, 0.17522)\n",
      "The confidence interval for DOT_Close does not include zero, suggesting a significant effect.\n",
      "\n",
      "\n",
      "Feature: DOT_Volume\n",
      "Zero Proportion: 0.085\n",
      "95% Confidence Interval: (-0.09663, 0.00000)\n",
      "The confidence interval for DOT_Volume includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: ADA_Close\n",
      "Zero Proportion: 0.021\n",
      "95% Confidence Interval: (0.00196, 0.12802)\n",
      "The confidence interval for ADA_Close does not include zero, suggesting a significant effect.\n",
      "\n",
      "\n",
      "Feature: ADA_Volume\n",
      "Zero Proportion: 0.238\n",
      "95% Confidence Interval: (-0.07232, 0.00000)\n",
      "The confidence interval for ADA_Volume includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: BNB_Close\n",
      "Zero Proportion: 0.228\n",
      "95% Confidence Interval: (0.00000, 0.13990)\n",
      "The confidence interval for BNB_Close includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: BNB_Volume\n",
      "Zero Proportion: 0.555\n",
      "95% Confidence Interval: (-0.03434, 0.00000)\n",
      "The confidence interval for BNB_Volume includes zero, suggesting no significant effect.\n",
      "The feature BNB_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: SOL_Close\n",
      "Zero Proportion: 0.000\n",
      "95% Confidence Interval: (0.07631, 0.18218)\n",
      "The confidence interval for SOL_Close does not include zero, suggesting a significant effect.\n",
      "\n",
      "\n",
      "Feature: SOL_Volume\n",
      "Zero Proportion: 0.037\n",
      "95% Confidence Interval: (-0.10113, 0.00000)\n",
      "The confidence interval for SOL_Volume includes zero, suggesting no significant effect.\n",
      "\n",
      "\n",
      "Feature: DOGE_Close\n",
      "Zero Proportion: 0.692\n",
      "95% Confidence Interval: (0.00000, 0.05901)\n",
      "The confidence interval for DOGE_Close includes zero, suggesting no significant effect.\n",
      "The feature DOGE_Close was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n",
      "Feature: DOGE_Volume\n",
      "Zero Proportion: 0.853\n",
      "95% Confidence Interval: (-0.00659, 0.00000)\n",
      "The confidence interval for DOGE_Volume includes zero, suggesting no significant effect.\n",
      "The feature DOGE_Volume was not selected in more than half of the bootstrap samples.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "\n",
    "# Assuming your DataFrame df_normalized and response Y are already defined as per your previous code\n",
    "\n",
    "# Set the alpha for the Lasso based on previous cross-validation results\n",
    "chosen_alpha = model.alpha_ # replace with your chosen alpha value from LassoCV\n",
    "\n",
    "# Initialize the Lasso model with the chosen alpha\n",
    "lasso_model = Lasso(alpha=chosen_alpha)\n",
    "\n",
    "# Define the number of bootstraps\n",
    "n_bootstraps = 10000\n",
    "\n",
    "# Store the bootstrapped coefficients\n",
    "bootstrap_coefficients = []\n",
    "\n",
    "# Perform the bootstrap resampling\n",
    "for i in range(n_bootstraps):\n",
    "    # Resample the data with replacement\n",
    "    X_boot, y_boot = resample(X, Y)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Define LassoCV with cross-validation to select alpha for each bootstrap sample\n",
    "    lasso_model = LassoCV(cv=tscv, normalize=False,  random_state=0) \n",
    "    lasso_model.fit(X_boot, y_boot)\n",
    "    \n",
    "    # Store the coefficients\n",
    "    bootstrap_coefficients.append(lasso_model.coef_)\n",
    "\n",
    "# Convert the list of coefficients to a DataFrame\n",
    "feature_names = df_normalized.drop(columns=['ETH_Price']).columns\n",
    "bootstrap_coefficients_df = pd.DataFrame(bootstrap_coefficients, columns=feature_names)\n",
    "\n",
    "# Calculate zero proportion and 95% confidence intervals for each coefficient\n",
    "zero_proportions = (bootstrap_coefficients_df == 0).mean()\n",
    "conf_intervals = np.percentile(bootstrap_coefficients_df, [2.5, 97.5], axis=0)\n",
    "\n",
    "# Output the results\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    zero_proportion = zero_proportions[feature_name]\n",
    "    ci_lower, ci_upper = conf_intervals[:, i]\n",
    "    \n",
    "    print(f\"Feature: {feature_name}\")\n",
    "    print(f\"Zero Proportion: {zero_proportion:.3f}\")\n",
    "    print(f\"95% Confidence Interval: ({ci_lower:.5f}, {ci_upper:.5f})\")\n",
    "    \n",
    "    # Interpretation based on the zero proportion and confidence interval\n",
    "    if ci_lower <= 0 <= ci_upper:\n",
    "        print(f\"The confidence interval for {feature_name} includes zero, suggesting no significant effect.\")\n",
    "    else:\n",
    "        print(f\"The confidence interval for {feature_name} does not include zero, suggesting a significant effect.\")\n",
    "    if zero_proportion > 0.5:\n",
    "        print(f\"The feature {feature_name} was not selected in more than half of the bootstrap samples.\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1181fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Token Name  Zero Proportion 95% Confidence Interval\n",
      "0                          ETH_Volume           0.0285      (0.00000, 0.33937)\n",
      "1                           BTC_Price           0.0248      (0.00169, 0.40475)\n",
      "2                          BTC_Volume           0.0688     (-0.17972, 0.00000)\n",
      "3            LINK_number_of_transfers           0.8051     (-0.00707, 0.00797)\n",
      "4                LINK_on_chain_volume           0.8522     (-0.00493, 0.00581)\n",
      "5                          LINK_Price           0.0000      (0.15666, 0.27313)\n",
      "6                         LINK_Volume           0.7656      (0.00000, 0.03795)\n",
      "7   GWEI_average_daily_gas_price_gwei           0.5096     (-0.00729, 0.02314)\n",
      "8            WBTC_number_of_transfers           0.2977     (-0.06620, 0.00000)\n",
      "9                WBTC_on_chain_volume           0.8236      (0.00000, 0.01071)\n",
      "10                         WBTC_Price           0.3648      (0.00000, 0.34269)\n",
      "11                        WBTC_Volume           0.4748      (0.00000, 0.05790)\n",
      "12           USDT_number_of_transfers           0.8514     (-0.00276, 0.01247)\n",
      "13               USDT_on_chain_volume           0.8628     (-0.00881, 0.01417)\n",
      "14                         USDT_Price           0.6839     (-0.01098, 0.02009)\n",
      "15                        USDT_Volume           0.9470     (-0.01176, 0.00000)\n",
      "16           USDC_number_of_transfers           0.6497     (-0.03105, 0.00000)\n",
      "17               USDC_on_chain_volume           0.6099     (-0.01886, 0.00000)\n",
      "18                         USDC_Price           0.2778     (-0.04696, 0.00000)\n",
      "19                        USDC_Volume           0.8191      (0.00000, 0.00441)\n",
      "20          MATIC_number_of_transfers           0.7322     (-0.01398, 0.01767)\n",
      "21              MATIC_on_chain_volume           0.9340     (-0.00223, 0.00000)\n",
      "22                        MATIC_Price           0.0030      (0.02324, 0.14613)\n",
      "23                       MATIC_Volume           0.3827     (-0.05296, 0.00000)\n",
      "24           SHIB_number_of_transfers           0.5440     (-0.01758, 0.00000)\n",
      "25               SHIB_on_chain_volume           0.7322      (0.00000, 0.00764)\n",
      "26                         SHIB_Price           0.6194     (-0.01797, 0.00098)\n",
      "27                        SHIB_Volume           0.7480     (-0.01277, 0.00000)\n",
      "28           WETH_number_of_transfers           0.5930     (-0.00641, 0.02964)\n",
      "29               WETH_on_chain_volume           0.5115     (-0.03620, 0.00000)\n",
      "30                        WETH_Volume           0.7732     (-0.00638, 0.00000)\n",
      "31            DAI_number_of_transfers           0.2361     (-0.06066, 0.00000)\n",
      "32                DAI_on_chain_volume           0.4795     (-0.02557, 0.00000)\n",
      "33                          DAI_Price           0.4606      (0.00000, 0.03687)\n",
      "34                         DAI_Volume           0.7966     (-0.01950, 0.01090)\n",
      "35               new_tokens_monthly_x           0.3326      (0.00000, 0.03669)\n",
      "36                           CPILFESL           0.5868     (-0.00151, 0.02205)\n",
      "37                         EUEPUINDXM           0.7066     (-0.01834, 0.01382)\n",
      "38                     CHNMAINLANDEPU           0.6188      (0.00000, 0.02173)\n",
      "39                       NASDAQ_Close           0.4927      (0.00000, 0.03223)\n",
      "40                      NASDAQ_Volume           0.7361     (-0.01248, 0.00733)\n",
      "41                          TNX_Close           0.6309     (-0.02002, 0.00624)\n",
      "42                         USEPUINDXD           0.7025     (-0.01547, 0.00455)\n",
      "43                                FFR           0.3829      (0.00000, 0.02592)\n",
      "44                          NIKKEI225           0.4537      (0.00000, 0.02743)\n",
      "45                       DCOILBRENTEU           0.5671     (-0.00095, 0.02162)\n",
      "46                              SP500           0.8097      (0.00000, 0.02142)\n",
      "47                          GOOGTREND           0.0405      (0.00000, 0.11640)\n",
      "48                          TRX_Close           0.0850      (0.00000, 0.07351)\n",
      "49                         TRX_Volume           0.8268     (-0.01061, 0.00792)\n",
      "50                          XRP_Close           0.1495      (0.00000, 0.06730)\n",
      "51                         XRP_Volume           0.7026     (-0.02704, 0.00000)\n",
      "52                          DOT_Close           0.0003      (0.04646, 0.17522)\n",
      "53                         DOT_Volume           0.0850     (-0.09663, 0.00000)\n",
      "54                          ADA_Close           0.0212      (0.00196, 0.12802)\n",
      "55                         ADA_Volume           0.2384     (-0.07232, 0.00000)\n",
      "56                          BNB_Close           0.2275      (0.00000, 0.13990)\n",
      "57                         BNB_Volume           0.5552     (-0.03434, 0.00000)\n",
      "58                          SOL_Close           0.0000      (0.07631, 0.18218)\n",
      "59                         SOL_Volume           0.0375     (-0.10113, 0.00000)\n",
      "60                         DOGE_Close           0.6924      (0.00000, 0.05901)\n",
      "61                        DOGE_Volume           0.8533     (-0.00659, 0.00000)\n"
     ]
    }
   ],
   "source": [
    "# Create lists to hold the data\n",
    "token_names = []\n",
    "zero_proportions_list = []\n",
    "conf_intervals_list = []\n",
    "\n",
    "# Populate the lists with the calculated values\n",
    "for feature_name in feature_names:\n",
    "    zero_proportion = zero_proportions[feature_name]\n",
    "    ci_lower, ci_upper = np.percentile(bootstrap_coefficients_df[feature_name], [2.5, 97.5])\n",
    "    \n",
    "    token_names.append(feature_name)\n",
    "    zero_proportions_list.append(zero_proportion)\n",
    "    conf_intervals_list.append(f\"({ci_lower:.5f}, {ci_upper:.5f})\")\n",
    "\n",
    "# Create a DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Token Name': token_names,\n",
    "    'Zero Proportion': zero_proportions_list,\n",
    "    '95% Confidence Interval': conf_intervals_list\n",
    "})\n",
    "\n",
    "# Show the DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "858adbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c60b246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv('/Users/gadimg/Library/Mobile Documents/com~apple~CloudDocs/PhD dis/third/Results/pct_change_results/robustness.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f979a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_main is your main merged dataset\n",
    "# and df_ether_transfers is your newly acquired Ether transfers dataset with columns ['Date', 'Transfers']\n",
    "df_main = pd.read_csv('/Users/gadimg/Library/Mobile Documents/com~apple~CloudDocs/PhD dis/third/Data/Merged_data/Full_with_otherCrypto-mothly.csv', parse_dates = ['Date'], dayfirst = True)\n",
    "df_ether_transfers = pd.read_csv('/Users/gadimg/Library/Mobile Documents/com~apple~CloudDocs/PhD dis/third/Data/EthereumBCData/Ether_transfers.csv', parse_dates = ['Date'], dayfirst = True)\n",
    "# Ensure the 'Date' column is in datetime format with dayfirst=True\n",
    "df_main['Date'] = pd.to_datetime(df_main['Date'], dayfirst=True)\n",
    "df_ether_transfers['Date'] = pd.to_datetime(df_ether_transfers['Date'], dayfirst=True)\n",
    "\n",
    "# Set 'Date' as the index for both DataFrames\n",
    "df_main.set_index('Date', inplace=True)\n",
    "df_ether_transfers.set_index('Date', inplace=True)\n",
    "\n",
    "# Create a date range that covers the full range of your main DataFrame\n",
    "date_range = pd.date_range(start='2020-08-20', end='2023-10-20')\n",
    "\n",
    "# Reindex the Ether transfers DataFrame to this date range\n",
    "df_ether_transfers = df_ether_transfers.reindex(date_range)\n",
    "\n",
    "# Interpolate the missing values\n",
    "df_ether_transfers['Transfers'] = df_ether_transfers['Transfers'].interpolate(method='time')\n",
    "\n",
    "# Merge the Ether transfers data into your main DataFrame\n",
    "df_merged = df_main.merge(df_ether_transfers, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# If the merged dataset has any NaNs in 'Transfers' from the date alignment, interpolate them\n",
    "df_merged['Transfers'] = df_merged['Transfers'].interpolate(method='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a23a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ETH_Price</th>\n",
       "      <th>ETH_Volume</th>\n",
       "      <th>BTC_Price</th>\n",
       "      <th>BTC_Volume</th>\n",
       "      <th>LINK_number_of_transfers</th>\n",
       "      <th>LINK_on_chain_volume</th>\n",
       "      <th>LINK_Price</th>\n",
       "      <th>LINK_Volume</th>\n",
       "      <th>GWEI_average_daily_gas_price_gwei</th>\n",
       "      <th>WBTC_number_of_transfers</th>\n",
       "      <th>...</th>\n",
       "      <th>DOT_Volume</th>\n",
       "      <th>ADA_Close</th>\n",
       "      <th>ADA_Volume</th>\n",
       "      <th>BNB_Close</th>\n",
       "      <th>BNB_Volume</th>\n",
       "      <th>SOL_Close</th>\n",
       "      <th>SOL_Volume</th>\n",
       "      <th>DOGE_Close</th>\n",
       "      <th>DOGE_Volume</th>\n",
       "      <th>Transfers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-08-20</th>\n",
       "      <td>416.439789</td>\n",
       "      <td>10043032427</td>\n",
       "      <td>11878.37207</td>\n",
       "      <td>2.017524e+10</td>\n",
       "      <td>27332</td>\n",
       "      <td>1.760189e+07</td>\n",
       "      <td>16.091297</td>\n",
       "      <td>1.441868e+09</td>\n",
       "      <td>116.484821</td>\n",
       "      <td>1862</td>\n",
       "      <td>...</td>\n",
       "      <td>48819867</td>\n",
       "      <td>0.133642</td>\n",
       "      <td>314037834</td>\n",
       "      <td>23.092575</td>\n",
       "      <td>240389084</td>\n",
       "      <td>3.208267</td>\n",
       "      <td>13756478</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>43536656</td>\n",
       "      <td>457774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-21</th>\n",
       "      <td>389.126343</td>\n",
       "      <td>11781796374</td>\n",
       "      <td>11592.48926</td>\n",
       "      <td>2.376243e+10</td>\n",
       "      <td>46596</td>\n",
       "      <td>4.071089e+07</td>\n",
       "      <td>13.800856</td>\n",
       "      <td>2.545003e+09</td>\n",
       "      <td>161.821743</td>\n",
       "      <td>1769</td>\n",
       "      <td>...</td>\n",
       "      <td>49313137</td>\n",
       "      <td>0.123760</td>\n",
       "      <td>363034924</td>\n",
       "      <td>22.128994</td>\n",
       "      <td>245533836</td>\n",
       "      <td>2.943668</td>\n",
       "      <td>11796736</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>60621268</td>\n",
       "      <td>478123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-22</th>\n",
       "      <td>395.835144</td>\n",
       "      <td>10131847985</td>\n",
       "      <td>11681.82520</td>\n",
       "      <td>2.022419e+10</td>\n",
       "      <td>31642</td>\n",
       "      <td>2.129189e+07</td>\n",
       "      <td>15.960192</td>\n",
       "      <td>1.966563e+09</td>\n",
       "      <td>107.424359</td>\n",
       "      <td>2542</td>\n",
       "      <td>...</td>\n",
       "      <td>485801096</td>\n",
       "      <td>0.125276</td>\n",
       "      <td>296099194</td>\n",
       "      <td>22.239820</td>\n",
       "      <td>208053381</td>\n",
       "      <td>3.020886</td>\n",
       "      <td>10296805</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>44884852</td>\n",
       "      <td>425725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-23</th>\n",
       "      <td>391.384491</td>\n",
       "      <td>8137303970</td>\n",
       "      <td>11664.84766</td>\n",
       "      <td>1.848206e+10</td>\n",
       "      <td>25341</td>\n",
       "      <td>1.601273e+07</td>\n",
       "      <td>15.174676</td>\n",
       "      <td>1.286304e+09</td>\n",
       "      <td>86.161309</td>\n",
       "      <td>2674</td>\n",
       "      <td>...</td>\n",
       "      <td>320749483</td>\n",
       "      <td>0.121595</td>\n",
       "      <td>232234230</td>\n",
       "      <td>21.870842</td>\n",
       "      <td>181970641</td>\n",
       "      <td>3.322431</td>\n",
       "      <td>11991402</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>39485548</td>\n",
       "      <td>427569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-24</th>\n",
       "      <td>408.144196</td>\n",
       "      <td>10328860398</td>\n",
       "      <td>11774.59570</td>\n",
       "      <td>2.068151e+10</td>\n",
       "      <td>23397</td>\n",
       "      <td>1.550895e+07</td>\n",
       "      <td>15.183360</td>\n",
       "      <td>1.148602e+09</td>\n",
       "      <td>98.932180</td>\n",
       "      <td>2043</td>\n",
       "      <td>...</td>\n",
       "      <td>407690171</td>\n",
       "      <td>0.124488</td>\n",
       "      <td>278021260</td>\n",
       "      <td>22.605852</td>\n",
       "      <td>192633739</td>\n",
       "      <td>3.344986</td>\n",
       "      <td>15785208</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>46470522</td>\n",
       "      <td>444316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-16</th>\n",
       "      <td>1600.534302</td>\n",
       "      <td>8846928526</td>\n",
       "      <td>28519.46680</td>\n",
       "      <td>2.783388e+10</td>\n",
       "      <td>8781</td>\n",
       "      <td>1.249359e+07</td>\n",
       "      <td>7.539671</td>\n",
       "      <td>2.618974e+08</td>\n",
       "      <td>14.850236</td>\n",
       "      <td>8120</td>\n",
       "      <td>...</td>\n",
       "      <td>104165497</td>\n",
       "      <td>0.251563</td>\n",
       "      <td>187099359</td>\n",
       "      <td>214.823959</td>\n",
       "      <td>521099239</td>\n",
       "      <td>23.982958</td>\n",
       "      <td>726713710</td>\n",
       "      <td>0.060114</td>\n",
       "      <td>255254744</td>\n",
       "      <td>481162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-17</th>\n",
       "      <td>1565.439575</td>\n",
       "      <td>5032686973</td>\n",
       "      <td>28415.74805</td>\n",
       "      <td>1.487253e+10</td>\n",
       "      <td>3924</td>\n",
       "      <td>1.497771e+07</td>\n",
       "      <td>7.332848</td>\n",
       "      <td>1.649857e+08</td>\n",
       "      <td>12.340605</td>\n",
       "      <td>5787</td>\n",
       "      <td>...</td>\n",
       "      <td>72585706</td>\n",
       "      <td>0.247115</td>\n",
       "      <td>131953756</td>\n",
       "      <td>211.643234</td>\n",
       "      <td>352621995</td>\n",
       "      <td>23.959318</td>\n",
       "      <td>520830665</td>\n",
       "      <td>0.059120</td>\n",
       "      <td>154163593</td>\n",
       "      <td>471803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-18</th>\n",
       "      <td>1563.749878</td>\n",
       "      <td>4354138855</td>\n",
       "      <td>28328.34180</td>\n",
       "      <td>1.272413e+10</td>\n",
       "      <td>3793</td>\n",
       "      <td>7.617895e+06</td>\n",
       "      <td>7.361921</td>\n",
       "      <td>1.500350e+08</td>\n",
       "      <td>12.820216</td>\n",
       "      <td>5208</td>\n",
       "      <td>...</td>\n",
       "      <td>62395387</td>\n",
       "      <td>0.243453</td>\n",
       "      <td>107992174</td>\n",
       "      <td>210.501038</td>\n",
       "      <td>275336939</td>\n",
       "      <td>23.432138</td>\n",
       "      <td>362606644</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>113718362</td>\n",
       "      <td>486108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-19</th>\n",
       "      <td>1567.457031</td>\n",
       "      <td>5035110867</td>\n",
       "      <td>28719.80664</td>\n",
       "      <td>1.444806e+10</td>\n",
       "      <td>4230</td>\n",
       "      <td>9.317814e+06</td>\n",
       "      <td>7.310982</td>\n",
       "      <td>1.434944e+08</td>\n",
       "      <td>13.178353</td>\n",
       "      <td>4486</td>\n",
       "      <td>...</td>\n",
       "      <td>78858521</td>\n",
       "      <td>0.246923</td>\n",
       "      <td>120374373</td>\n",
       "      <td>211.144165</td>\n",
       "      <td>281310969</td>\n",
       "      <td>24.937920</td>\n",
       "      <td>663249208</td>\n",
       "      <td>0.058883</td>\n",
       "      <td>119806282</td>\n",
       "      <td>473733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-20</th>\n",
       "      <td>1604.666870</td>\n",
       "      <td>6747486127</td>\n",
       "      <td>29682.94922</td>\n",
       "      <td>2.153613e+10</td>\n",
       "      <td>3305</td>\n",
       "      <td>1.002584e+07</td>\n",
       "      <td>7.581278</td>\n",
       "      <td>2.168861e+08</td>\n",
       "      <td>16.980754</td>\n",
       "      <td>5520</td>\n",
       "      <td>...</td>\n",
       "      <td>103783716</td>\n",
       "      <td>0.251138</td>\n",
       "      <td>137878642</td>\n",
       "      <td>212.300842</td>\n",
       "      <td>335590168</td>\n",
       "      <td>27.033674</td>\n",
       "      <td>910991447</td>\n",
       "      <td>0.059965</td>\n",
       "      <td>154146216</td>\n",
       "      <td>548227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1163 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ETH_Price   ETH_Volume    BTC_Price    BTC_Volume  \\\n",
       "2020-08-20   416.439789  10043032427  11878.37207  2.017524e+10   \n",
       "2020-08-21   389.126343  11781796374  11592.48926  2.376243e+10   \n",
       "2020-08-22   395.835144  10131847985  11681.82520  2.022419e+10   \n",
       "2020-08-23   391.384491   8137303970  11664.84766  1.848206e+10   \n",
       "2020-08-24   408.144196  10328860398  11774.59570  2.068151e+10   \n",
       "...                 ...          ...          ...           ...   \n",
       "2023-10-16  1600.534302   8846928526  28519.46680  2.783388e+10   \n",
       "2023-10-17  1565.439575   5032686973  28415.74805  1.487253e+10   \n",
       "2023-10-18  1563.749878   4354138855  28328.34180  1.272413e+10   \n",
       "2023-10-19  1567.457031   5035110867  28719.80664  1.444806e+10   \n",
       "2023-10-20  1604.666870   6747486127  29682.94922  2.153613e+10   \n",
       "\n",
       "            LINK_number_of_transfers  LINK_on_chain_volume  LINK_Price  \\\n",
       "2020-08-20                     27332          1.760189e+07   16.091297   \n",
       "2020-08-21                     46596          4.071089e+07   13.800856   \n",
       "2020-08-22                     31642          2.129189e+07   15.960192   \n",
       "2020-08-23                     25341          1.601273e+07   15.174676   \n",
       "2020-08-24                     23397          1.550895e+07   15.183360   \n",
       "...                              ...                   ...         ...   \n",
       "2023-10-16                      8781          1.249359e+07    7.539671   \n",
       "2023-10-17                      3924          1.497771e+07    7.332848   \n",
       "2023-10-18                      3793          7.617895e+06    7.361921   \n",
       "2023-10-19                      4230          9.317814e+06    7.310982   \n",
       "2023-10-20                      3305          1.002584e+07    7.581278   \n",
       "\n",
       "             LINK_Volume  GWEI_average_daily_gas_price_gwei  \\\n",
       "2020-08-20  1.441868e+09                         116.484821   \n",
       "2020-08-21  2.545003e+09                         161.821743   \n",
       "2020-08-22  1.966563e+09                         107.424359   \n",
       "2020-08-23  1.286304e+09                          86.161309   \n",
       "2020-08-24  1.148602e+09                          98.932180   \n",
       "...                  ...                                ...   \n",
       "2023-10-16  2.618974e+08                          14.850236   \n",
       "2023-10-17  1.649857e+08                          12.340605   \n",
       "2023-10-18  1.500350e+08                          12.820216   \n",
       "2023-10-19  1.434944e+08                          13.178353   \n",
       "2023-10-20  2.168861e+08                          16.980754   \n",
       "\n",
       "            WBTC_number_of_transfers  ...  DOT_Volume  ADA_Close  ADA_Volume  \\\n",
       "2020-08-20                      1862  ...    48819867   0.133642   314037834   \n",
       "2020-08-21                      1769  ...    49313137   0.123760   363034924   \n",
       "2020-08-22                      2542  ...   485801096   0.125276   296099194   \n",
       "2020-08-23                      2674  ...   320749483   0.121595   232234230   \n",
       "2020-08-24                      2043  ...   407690171   0.124488   278021260   \n",
       "...                              ...  ...         ...        ...         ...   \n",
       "2023-10-16                      8120  ...   104165497   0.251563   187099359   \n",
       "2023-10-17                      5787  ...    72585706   0.247115   131953756   \n",
       "2023-10-18                      5208  ...    62395387   0.243453   107992174   \n",
       "2023-10-19                      4486  ...    78858521   0.246923   120374373   \n",
       "2023-10-20                      5520  ...   103783716   0.251138   137878642   \n",
       "\n",
       "             BNB_Close  BNB_Volume  SOL_Close  SOL_Volume  DOGE_Close  \\\n",
       "2020-08-20   23.092575   240389084   3.208267    13756478    0.003459   \n",
       "2020-08-21   22.128994   245533836   2.943668    11796736    0.003393   \n",
       "2020-08-22   22.239820   208053381   3.020886    10296805    0.003442   \n",
       "2020-08-23   21.870842   181970641   3.322431    11991402    0.003414   \n",
       "2020-08-24   22.605852   192633739   3.344986    15785208    0.003418   \n",
       "...                ...         ...        ...         ...         ...   \n",
       "2023-10-16  214.823959   521099239  23.982958   726713710    0.060114   \n",
       "2023-10-17  211.643234   352621995  23.959318   520830665    0.059120   \n",
       "2023-10-18  210.501038   275336939  23.432138   362606644    0.058673   \n",
       "2023-10-19  211.144165   281310969  24.937920   663249208    0.058883   \n",
       "2023-10-20  212.300842   335590168  27.033674   910991447    0.059965   \n",
       "\n",
       "            DOGE_Volume  Transfers  \n",
       "2020-08-20     43536656     457774  \n",
       "2020-08-21     60621268     478123  \n",
       "2020-08-22     44884852     425725  \n",
       "2020-08-23     39485548     427569  \n",
       "2020-08-24     46470522     444316  \n",
       "...                 ...        ...  \n",
       "2023-10-16    255254744     481162  \n",
       "2023-10-17    154163593     471803  \n",
       "2023-10-18    113718362     486108  \n",
       "2023-10-19    119806282     473733  \n",
       "2023-10-20    154146216     548227  \n",
       "\n",
       "[1163 rows x 60 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39c4ef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ljung-Box test statistic: lb_stat, p-value: lb_pvalue\n",
      "    lb_stat  lb_pvalue\n",
      "1  0.010792   0.917261\n",
      "ADF test statistic: -9.752604493029285, p-value: 7.94209535770238e-17\n",
      "{'Lagrange Multiplier Statistic': 16.09244828276706, 'p-value': 6.0324109186215184e-05, 'f-value': 18.79520852933282, 'f p-value': 3.530715538586004e-05}\n",
      "Diebold-Mariano test statistic: -3.8429940554601294, p-value: 0.0002147875367113361\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_breuschpagan\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import t\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# Assuming 'actuals', 'predictionsLasso', and 'ols_predictions' are already defined\n",
    "\n",
    "# Calculate squared forecast errors\n",
    "squared_errors_lasso = (actuals - predictionsLasso) ** 2\n",
    "squared_errors_ols = (actuals - ols_predictions) ** 2\n",
    "\n",
    "# Compute differences in squared errors\n",
    "error_diffs = squared_errors_lasso - squared_errors_ols\n",
    "\n",
    "# Check for autocorrelation in error differences\n",
    "stat, p_value_autocorr = acorr_ljungbox(error_diffs, lags=[1], return_df=True)\n",
    "print(f\"Ljung-Box test statistic: {stat}, p-value: {p_value_autocorr}\")\n",
    "print(acorr_ljungbox(error_diffs, lags=[1], return_df=True))\n",
    "# Check for stationarity\n",
    "adf_stat, adf_p_value, _, _, _, _ = adfuller(error_diffs)\n",
    "print(f\"ADF test statistic: {adf_stat}, p-value: {adf_p_value}\")\n",
    "\n",
    "# Check for homoscedasticity (constant variance)\n",
    "bp_test = het_breuschpagan(error_diffs, add_constant(actuals))\n",
    "labels = ['Lagrange Multiplier Statistic', 'p-value', 'f-value', 'f p-value']\n",
    "print(dict(zip(labels, bp_test)))\n",
    "\n",
    "# Diebold-Mariano test\n",
    "n = len(error_diffs)\n",
    "mean_diff = np.mean(error_diffs)\n",
    "std_diff = np.sqrt(np.var(error_diffs, ddof=1) / n)\n",
    "DM_stat = mean_diff / std_diff\n",
    "p_value = 2 * (1 - t.cdf(np.abs(DM_stat), df=n-1))\n",
    "\n",
    "print(f\"Diebold-Mariano test statistic: {DM_stat}, p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4e906226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso vs AR(1):\n",
      "Ljung-Box test: lb_stat, p-value: lb_pvalue\n",
      "    lb_stat  lb_pvalue\n",
      "1  0.492798   0.482682\n",
      "ADF test: -9.276740618341371, p-value: 1.2829378544867169e-15\n",
      "Breusch-Pagan test: {'Lagrange Multiplier Statistic': 16.919267541010775, 'p-value': 3.9003667245845794e-05, 'f-value': 19.95755417584372, 'f p-value': 2.124818547026269e-05}\n",
      "\n",
      "Diebold-Mariano test (Lasso vs. AR(1)): DM_stat = -3.126844371445629 , p-value = 0.002320027555980486\n"
     ]
    }
   ],
   "source": [
    "# Calculating squared forecast errors for AR(1) model\n",
    "squared_errors_ar = (actuals - np.array(ar_predictions)) ** 2\n",
    "\n",
    "# Compute differences in squared errors\n",
    "error_diffs_lasso_ar = squared_errors_lasso - squared_errors_ar\n",
    "\n",
    "\n",
    "# Check for autocorrelation, stationarity, and homoscedasticity for Lasso vs. AR(1)\n",
    "print(\"Lasso vs AR(1):\")\n",
    "stat, p_value_autocorr = acorr_ljungbox(error_diffs_lasso_ar, lags=[1])\n",
    "\n",
    "print(f\"Ljung-Box test: {stat}, p-value: {p_value_autocorr}\")\n",
    "print(acorr_ljungbox(error_diffs_lasso_ar, lags=[1]))\n",
    "adf_stat, adf_p_value, _, _, _, _ = adfuller(error_diffs_lasso_ar)\n",
    "print(f\"ADF test: {adf_stat}, p-value: {adf_p_value}\")\n",
    "bp_test = het_breuschpagan(error_diffs_lasso_ar, add_constant(actuals))\n",
    "print(\"Breusch-Pagan test:\", dict(zip(labels, bp_test)))\n",
    "\n",
    "\n",
    "# Diebold-Mariano test for Lasso vs. AR(1)\n",
    "n = len(error_diffs_lasso_ar)\n",
    "mean_diff = np.mean(error_diffs_lasso_ar)\n",
    "std_diff = np.sqrt(np.var(error_diffs_lasso_ar, ddof=1) / n)\n",
    "DM_stat = mean_diff / std_diff\n",
    "p_value = 2 * (1 - t.cdf(np.abs(DM_stat), df=n-1))\n",
    "print(\"\\nDiebold-Mariano test (Lasso vs. AR(1)): DM_stat =\", DM_stat, \", p-value =\", p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f0e6ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso vs OLS:\n",
      "Ljung-Box test: lb_stat, p-value: lb_pvalue\n",
      "    lb_stat  lb_pvalue\n",
      "1  1.036633   0.308606\n",
      "ADF test: -8.900537946860576, p-value: 1.1746671345071197e-14\n",
      "Breusch-Pagan test: {'Lagrange Multiplier Statistic': 11.382833169040508, 'p-value': 0.0007412597073805801, 'f-value': 12.58805365210852, 'f p-value': 0.0005977051823367553}\n",
      "\n",
      "Diebold-Mariano test (Lasso vs OLS): DM_stat = -5.5389645836137325 , p-value = 2.510803394084604e-07\n",
      "Lasso vs AR:\n",
      "Ljung-Box test: lb_stat, p-value: lb_pvalue\n",
      "    lb_stat  lb_pvalue\n",
      "1  2.195939   0.138375\n",
      "ADF test: -8.623324447025555, p-value: 6.022496329945389e-14\n",
      "Breusch-Pagan test: {'Lagrange Multiplier Statistic': 11.123190041581232, 'p-value': 0.0008525502966521146, 'f-value': 12.264983684551169, 'f p-value': 0.0006967762517933656}\n",
      "\n",
      "Diebold-Mariano test (Lasso vs AR): DM_stat = -3.77264892751682 , p-value = 0.0002751945556578317\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'actuals', 'predictionsLasso', 'ols_predictions', and 'ar_predictions' are defined\n",
    "#testing MAE for LASSO vs. OLS\n",
    "# Calculate absolute errors\n",
    "abs_errors_lasso = np.abs(actuals - predictionsLasso)\n",
    "abs_errors_ols = np.abs(actuals - ols_predictions)\n",
    "abs_errors_ar = np.abs(actuals - ar_predictions)\n",
    "\n",
    "# Compute differences in absolute errors\n",
    "error_diffs_mae_lasso_ols = abs_errors_lasso - abs_errors_ols\n",
    "error_diffs_mae_lasso_ar = abs_errors_lasso - abs_errors_ar\n",
    "\n",
    "# Check for autocorrelation in error differences (Lasso vs OLS)\n",
    "print(\"Lasso vs OLS:\")\n",
    "stat, p_value_autocorr = acorr_ljungbox(error_diffs_mae_lasso_ols, lags=[1])\n",
    "print(f\"Ljung-Box test: {stat}, p-value: {p_value_autocorr}\")\n",
    "print(acorr_ljungbox(error_diffs_mae_lasso_ols, lags=[1]))\n",
    "# Check for stationarity (Lasso vs OLS)\n",
    "adf_stat, adf_p_value, _, _, _, _ = adfuller(error_diffs_mae_lasso_ols)\n",
    "print(f\"ADF test: {adf_stat}, p-value: {adf_p_value}\")\n",
    "\n",
    "# Check for homoscedasticity (Lasso vs OLS)\n",
    "bp_test = het_breuschpagan(error_diffs_mae_lasso_ols, add_constant(actuals))\n",
    "labels = ['Lagrange Multiplier Statistic', 'p-value', 'f-value', 'f p-value']\n",
    "print(\"Breusch-Pagan test:\", dict(zip(labels, bp_test)))\n",
    "\n",
    "# Diebold-Mariano test for MAE (Lasso vs OLS)\n",
    "n = len(error_diffs_mae_lasso_ols)\n",
    "mean_diff = np.mean(error_diffs_mae_lasso_ols)\n",
    "std_diff = np.sqrt(np.var(error_diffs_mae_lasso_ols, ddof=1) / n)\n",
    "DM_stat = mean_diff / std_diff\n",
    "p_value = 2 * (1 - t.cdf(np.abs(DM_stat), df=n-1))\n",
    "print(\"\\nDiebold-Mariano test (Lasso vs OLS): DM_stat =\", DM_stat, \", p-value =\", p_value)\n",
    "\n",
    "# Repeat the same steps for Lasso vs AR by replacing 'error_diffs_mae_lasso_ols' with 'error_diffs_mae_lasso_ar'\n",
    "\n",
    "# Check for autocorrelation in error differences (Lasso vs AR)\n",
    "print(\"Lasso vs AR:\")\n",
    "stat, p_value_autocorr = acorr_ljungbox(error_diffs_mae_lasso_ar, lags=[1])\n",
    "print(f\"Ljung-Box test: {stat}, p-value: {p_value_autocorr}\")\n",
    "print(acorr_ljungbox(error_diffs_mae_lasso_ar, lags=[1]))\n",
    "# Check for stationarity (Lasso vs OLS)\n",
    "adf_stat, adf_p_value, _, _, _, _ = adfuller(error_diffs_mae_lasso_ar)\n",
    "print(f\"ADF test: {adf_stat}, p-value: {adf_p_value}\")\n",
    "\n",
    "# Check for homoscedasticity (Lasso vs AR)\n",
    "bp_test = het_breuschpagan(error_diffs_mae_lasso_ar, add_constant(actuals))\n",
    "labels = ['Lagrange Multiplier Statistic', 'p-value', 'f-value', 'f p-value']\n",
    "print(\"Breusch-Pagan test:\", dict(zip(labels, bp_test)))\n",
    "\n",
    "# Diebold-Mariano test for MAE (Lasso vs AR)\n",
    "n = len(error_diffs_mae_lasso_ar)\n",
    "mean_diff = np.mean(error_diffs_mae_lasso_ar)\n",
    "std_diff = np.sqrt(np.var(error_diffs_mae_lasso_ar, ddof=1) / n)\n",
    "DM_stat = mean_diff / std_diff\n",
    "p_value = 2 * (1 - t.cdf(np.abs(DM_stat), df=n-1))\n",
    "print(\"\\nDiebold-Mariano test (Lasso vs AR): DM_stat =\", DM_stat, \", p-value =\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "875ef36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_train_ar is your training data\n",
    "lags = 15  # You can adjust the number of lags you want to inspect\n",
    "pacf_values = sm.tsa.stattools.pacf(y_train_ar, nlags=lags)\n",
    "\n",
    "# Plotting the PACF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.stem(range(len(pacf_values)), pacf_values, use_line_collection=True)\n",
    "plt.hlines([0.05, -0.05], xmin=0, xmax=lags, colors='r', linestyles='dashed')  # Significance lines\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('PACF')\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "955aef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Original ETH_Price  Predicted ETH_Price\n",
      "0            -0.033473            -0.028518\n",
      "1            -0.004063            -0.002500\n",
      "2            -0.004030            -0.013844\n",
      "3            -0.006257             0.017991\n",
      "4             0.000000             0.003251\n",
      "..                 ...                  ...\n",
      "95            0.027255             0.035204\n",
      "96           -0.021927            -0.011313\n",
      "97           -0.001079            -0.002303\n",
      "98            0.002371             0.010170\n",
      "99            0.023739             0.030723\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_eth_price = scaler.mean_[df_returns.columns.get_loc('ETH_Price')]\n",
    "std_eth_price = scaler.scale_[df_returns.columns.get_loc('ETH_Price')]\n",
    "predictions_original_scale = [p * std_eth_price + mean_eth_price for p in predictionsLasso]\n",
    "\n",
    "# Extract actuals from df_returns (already in original scale)\n",
    "actuals = df_returns['ETH_Price'].iloc[start:].values\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original ETH_Price': actuals,\n",
    "    'Predicted ETH_Price': predictions_original_scale\n",
    "})\n",
    "\n",
    "# Print the comparison DataFrame\n",
    "print(comparison_df)\n",
    "\n",
    "# Plotting the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(comparison_df['Original ETH_Price'].values, label='Original ETH_Price', color='blue')\n",
    "plt.plot(comparison_df['Predicted ETH_Price'].values, label='Predicted ETH_Price', color='red', linestyle='--')\n",
    "plt.title('Comparison of Original and Predicted ETH_Price')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('ETH_Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d2493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
